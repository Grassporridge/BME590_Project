{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BME 590 \u2014 Synthetic Microbiome Data Generator v3\n",
    "\n",
    "gLV with regime switching, mechanistic extensions, realistic sequencing observation model, and full benchmark infrastructure.\n",
    "\n",
    "### Fixes Applied\n",
    "- **#9** Absorbing-zero: immigration (propagule rain) in ALL RHS functions\n",
    "- **#10** Hidden/cumulative trigger heterogeneity: per-community parameter jitter\n",
    "- **#11** Single-gLV baseline: NLS replaces finite-difference ridge\n",
    "- **#12** Ground-truth regime parameters exported\n",
    "- **#13** Per-community switch-time estimation in metadata\n",
    "\n",
    "### New Functionality\n",
    "- Environmental drift scenario (non-stationary parameters)\n",
    "- Global carrying capacity constraint\n",
    "- Allee effects (cooperative growth at low density)\n",
    "- Shannon diversity tracking\n",
    "- Comprehensive validation suite\n",
    "- 12 registered scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports & Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "BME 590 \u2014 Synthetic Microbiome Data Generator v3\n",
    "=================================================\n",
    "gLV with regime switching, mechanistic extensions, realistic sequencing\n",
    "observation model, and full benchmark infrastructure.\n",
    "\n",
    "Fixes applied (from issue list):\n",
    "  - Absorbing-zero: immigration (propagule rain) in ALL RHS functions\n",
    "  - Hidden/cumulative trigger heterogeneity: per-community parameter jitter\n",
    "  - Single-gLV baseline: gradient-matching replaced with proper NLS\n",
    "  - Ground-truth regime parameters exported\n",
    "  - Per-community switch-time estimation in metadata\n",
    "  - Environmental drift scenario added\n",
    "  - Carrying-capacity constraint added\n",
    "  - Comprehensive validation suite\n",
    "\n",
    "Additions beyond checklist:\n",
    "  - Allee effects (cooperative growth at low density)\n",
    "  - pH / environmental coupling\n",
    "  - Lotka-Volterra with functional response (Type II Holling)\n",
    "  - Compositional (CLR) export for downstream analysis\n",
    "  - Shannon diversity tracking\n",
    "  - Pairwise community distance matrix export\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a70  IMPORTS & GLOBAL CONFIGURATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.optimize import least_squares\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "Number = Union[float, int]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GeneratorConfig:\n",
    "    \"\"\"Central configuration object governing all generator behaviour.\"\"\"\n",
    "\n",
    "    # \u2500\u2500 Reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    seed: int = 42\n",
    "    dataset_version: str = \"v3\"\n",
    "\n",
    "    # \u2500\u2500 Core simulation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    n_species: int = 5\n",
    "    n_metabolites: int = 1\n",
    "\n",
    "    t_start: float = 0.0\n",
    "    t_end: float = 20.0\n",
    "    n_timepoints: int = 101\n",
    "\n",
    "    # \u2500\u2500 Scenario \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    scenario: str = \"baseline_glv_stable\"\n",
    "\n",
    "    # \u2500\u2500 Ecological matrix structure \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    A_structure: str = \"sparse\"\n",
    "    A_structure_kwargs: dict = field(default_factory=lambda: dict(\n",
    "        p=0.15, offdiag_std=0.03, diag_range=(-0.6, -0.2)))\n",
    "    A_structure_kwargs2: Optional[dict] = None\n",
    "    stability_margin: float = 0.03\n",
    "\n",
    "    # \u2500\u2500 Hierarchical variation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    enable_hierarchical: bool = True\n",
    "    sigma_r: float = 0.15\n",
    "    sigma_A: float = 0.05\n",
    "\n",
    "    # \u2500\u2500 Regime switching \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    regime_distance: float = 1.0\n",
    "    t_switch: float = 10.0\n",
    "    epsilon: float = 1.0\n",
    "    smooth_k: float = 10.0\n",
    "    per_comm_switch: bool = False\n",
    "    switch_spread: float = 3.0\n",
    "\n",
    "    # \u2500\u2500 Hidden trigger \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    k_u: float = 0.4\n",
    "    u0: float = 0.0\n",
    "    theta: float = 0.5\n",
    "    # Per-community heterogeneity for hidden/cumulative triggers\n",
    "    trigger_sigma_theta: float = 0.0    # std of per-community theta jitter\n",
    "    trigger_sigma_k_u: float = 0.0      # std of per-community k_u jitter\n",
    "\n",
    "    # \u2500\u2500 Cumulative trigger \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    c1: float = 1.0\n",
    "    M_init: float = 0.05\n",
    "    a0: float = 0.0\n",
    "    idx_M: Optional[List[int]] = None\n",
    "\n",
    "    # \u2500\u2500 Resource dynamics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    enable_forcing: bool = False\n",
    "    diet_shocks: Optional[List[Dict]] = None\n",
    "    export_resources: bool = False\n",
    "\n",
    "    # \u2500\u2500 Bistability \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    bistable_saddle_dist: float = 0.3\n",
    "    bistable_comp_strength: float = 0.25\n",
    "    bistable_fac_strength: float = 0.02\n",
    "\n",
    "    # \u2500\u2500 Dormancy \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    dormancy_rate: float = 0.05\n",
    "    revival_rate: float = 0.10\n",
    "    dormancy_threshold: float = 0.01\n",
    "\n",
    "    # \u2500\u2500 Antibiotic pulse \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    enable_antibiotic: bool = False\n",
    "    antibiotic_start: float = 8.0\n",
    "    antibiotic_duration: float = 3.0\n",
    "    antibiotic_kill_rates: Optional[List[float]] = None\n",
    "\n",
    "    # \u2500\u2500 Bloom \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    enable_bloom: bool = False\n",
    "    bloom_species: int = 0\n",
    "    bloom_start: float = 5.0\n",
    "    bloom_boost: float = 2.0\n",
    "    bloom_duration: float = 2.0\n",
    "\n",
    "    # \u2500\u2500 Environmental drift \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    enable_drift: bool = False\n",
    "    drift_rate_r: float = 0.01       # magnitude of r drift per unit time\n",
    "    drift_rate_A: float = 0.005      # magnitude of A drift per unit time\n",
    "\n",
    "    # \u2500\u2500 Carrying capacity \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    enable_carrying_cap: bool = False\n",
    "    carrying_capacity: float = 2.0   # max total biomass\n",
    "\n",
    "    # \u2500\u2500 Allee effects \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    enable_allee: bool = False\n",
    "    allee_threshold: float = 0.005   # below this, growth is penalised\n",
    "\n",
    "    # \u2500\u2500 Observation model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    observation_mode: str = \"continuous\"\n",
    "    library_size_mean: float = 1e4\n",
    "    library_size_sigma: float = 0.6\n",
    "    dm_alpha_scale: float = 100.0\n",
    "    enable_dropout: bool = False\n",
    "    detection_limit: float = 2.0\n",
    "    n_replicates: int = 1\n",
    "\n",
    "    # \u2500\u2500 Sampling design \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    irregular_sampling: bool = False\n",
    "    irregular_keep_frac: float = 0.6\n",
    "    missing_rate: float = 0.0\n",
    "\n",
    "    # \u2500\u2500 Immigration (propagule rain) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    immigration_rate: float = 1e-4\n",
    "    immigration_scale: float = 1.0\n",
    "\n",
    "    # \u2500\u2500 Exports \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    export_latent: bool = True\n",
    "    export_regimes: bool = True\n",
    "    export_ground_truth_params: bool = True\n",
    "\n",
    "\n",
    "CONFIG = GeneratorConfig()\n",
    "print(\"CONFIG initialised. Scenario:\", CONFIG.scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNG Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a71  RNG HIERARCHY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def make_rng(stream_name: str, base_seed: int = None) -> np.random.Generator:\n",
    "    if base_seed is None:\n",
    "        base_seed = CONFIG.seed\n",
    "    offset = abs(hash(stream_name)) % (2**31)\n",
    "    return np.random.default_rng(base_seed + offset)\n",
    "\n",
    "\n",
    "RNG = dict(\n",
    "    params      = make_rng(\"params\"),\n",
    "    simulation  = make_rng(\"simulation\"),\n",
    "    observation = make_rng(\"observation\"),\n",
    "    sampling    = make_rng(\"sampling\"),\n",
    "    splits      = make_rng(\"splits\"),\n",
    ")\n",
    "\n",
    "def get_rng(name: str = \"simulation\") -> np.random.Generator:\n",
    "    return RNG[name]\n",
    "\n",
    "# Determinism check\n",
    "assert np.allclose(make_rng(\"test\").normal(size=3), make_rng(\"test\").normal(size=3))\n",
    "print(\"RNG hierarchy OK. Streams:\", list(RNG.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameter Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a72  PARAMETER SAMPLERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def _to_array(x, shape):\n",
    "    arr = np.asarray(x)\n",
    "    if arr.ndim == 0:\n",
    "        return np.broadcast_to(arr, shape).astype(float)\n",
    "    if arr.ndim == 1 and arr.shape[0] == shape[0]:\n",
    "        return np.broadcast_to(\n",
    "            arr.reshape(shape[0], *([1]*(len(shape)-1))), shape\n",
    "        ).astype(float)\n",
    "    return np.broadcast_to(arr, shape).astype(float)\n",
    "\n",
    "\n",
    "def generate_gaussian_params_with_diag(\n",
    "    n_species, vec_mean=0., vec_std=1., vec_bounds=(-np.inf, np.inf),\n",
    "    mat_mean=0., mat_std=1., mat_bounds=(-np.inf, np.inf),\n",
    "    diag_mean=0., diag_std=1., diag_bounds=(-np.inf, np.inf),\n",
    "    seed=None,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = n_species\n",
    "    v = np.clip(\n",
    "        rng.normal(_to_array(vec_mean, (n,)), _to_array(vec_std, (n,)), n),\n",
    "        *vec_bounds\n",
    "    )\n",
    "    M = np.clip(\n",
    "        rng.normal(_to_array(mat_mean, (n, n)), _to_array(mat_std, (n, n)), (n, n)),\n",
    "        *mat_bounds\n",
    "    )\n",
    "    d_mean = np.asarray(diag_mean)\n",
    "    d_std = np.asarray(diag_std)\n",
    "    dm = d_mean * np.ones(n) if d_mean.ndim == 0 else d_mean\n",
    "    ds = d_std * np.ones(n) if d_std.ndim == 0 else d_std\n",
    "    np.fill_diagonal(M, np.clip(rng.normal(dm, ds, n), *diag_bounds))\n",
    "    return v, M\n",
    "\n",
    "\n",
    "print(\"Parameter samplers defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Structured Matrices & Stable Regime Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a73  STRUCTURED MATRICES & STABLE REGIME BUILDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def generate_A_sparse(n, p=0.15, diag_range=(-0.6, -0.15),\n",
    "                      offdiag_mean=0., offdiag_std=0.03, rng=None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    A = np.zeros((n, n))\n",
    "    A[np.diag_indices(n)] = rng.uniform(*diag_range, size=n)\n",
    "    mask = rng.random((n, n)) < p\n",
    "    np.fill_diagonal(mask, False)\n",
    "    A[mask] = rng.normal(offdiag_mean, offdiag_std, mask.sum())\n",
    "    return A\n",
    "\n",
    "\n",
    "def generate_A_modular(n, n_blocks=2, p_in=0.25, p_out=0.05,\n",
    "                        diag_range=(-0.6, -0.15), offdiag_std=0.03, rng=None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    A = np.zeros((n, n))\n",
    "    A[np.diag_indices(n)] = rng.uniform(*diag_range, size=n)\n",
    "    blks = np.floor(np.linspace(0, n_blocks, n, endpoint=False)).astype(int)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            p = p_in if blks[i] == blks[j] else p_out\n",
    "            if rng.random() < p:\n",
    "                A[i, j] = rng.normal(0, offdiag_std)\n",
    "    return A\n",
    "\n",
    "\n",
    "def generate_A_lowrank_sparse(n, rank=2, p_sparse=0.10,\n",
    "                               diag_range=(-0.6, -0.15),\n",
    "                               lowrank_scale=0.02, sparse_std=0.02, rng=None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    A = lowrank_scale * (rng.normal(0, 1, (n, rank)) @ rng.normal(0, 1, (rank, n)))\n",
    "    mask = rng.random((n, n)) < p_sparse\n",
    "    np.fill_diagonal(mask, False)\n",
    "    A[mask] += rng.normal(0, sparse_std, mask.sum())\n",
    "    A[np.diag_indices(n)] = rng.uniform(*diag_range, size=n)\n",
    "    return A\n",
    "\n",
    "\n",
    "def construct_stable_regime(n_species, structure=\"sparse\", x_star=None,\n",
    "                             stability_margin=0.03, rng=None,\n",
    "                             structure_kwargs=None,\n",
    "                             eps_shift=1e-3, x_floor=1e-12):\n",
    "    \"\"\"\n",
    "    Build (r, A, x*) such that:\n",
    "      - r = -A x*  (x* is equilibrium)\n",
    "      - max Re(eig(J(x*))) < -stability_margin\n",
    "    Diagonal shift applied if necessary.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    kw = {} if structure_kwargs is None else dict(structure_kwargs)\n",
    "\n",
    "    if x_star is None:\n",
    "        x_star = rng.lognormal(-3.5, 0.7, n_species)\n",
    "    x_star = np.asarray(x_star, dtype=float)\n",
    "\n",
    "    builders = {\n",
    "        \"sparse\": generate_A_sparse,\n",
    "        \"modular\": generate_A_modular,\n",
    "        \"lowrank_sparse\": generate_A_lowrank_sparse,\n",
    "    }\n",
    "    if structure not in builders:\n",
    "        raise ValueError(f\"Unknown structure '{structure}'\")\n",
    "    A = builders[structure](n_species, rng=rng, **kw)\n",
    "\n",
    "    r = -(A @ x_star)\n",
    "    J = np.diag(x_star) @ A\n",
    "    max_real = float(np.max(np.real(np.linalg.eigvals(J))))\n",
    "\n",
    "    if max_real >= -stability_margin:\n",
    "        min_x = max(float(np.min(np.maximum(x_star, x_floor))), x_floor)\n",
    "        delta = (max_real + stability_margin + eps_shift) / min_x\n",
    "        A = A - delta * np.eye(n_species)\n",
    "        r = -(A @ x_star)\n",
    "\n",
    "    return r, A, x_star\n",
    "\n",
    "\n",
    "def sample_hierarchical_community_params(r_base, A_base,\n",
    "                                          sigma_r=0.15, sigma_A=0.05,\n",
    "                                          rng=None, present_mask=None):\n",
    "    \"\"\"Draw community-specific (r, A) around shared base parameters.\"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    r_base = np.asarray(r_base, dtype=float)\n",
    "    A_base = np.asarray(A_base, dtype=float)\n",
    "    n = r_base.shape[0]\n",
    "    r_n = rng.normal(0, sigma_r, n)\n",
    "    A_n = rng.normal(0, sigma_A, (n, n))\n",
    "    if present_mask is not None:\n",
    "        mask = np.asarray(present_mask, dtype=bool)\n",
    "        r_n[~mask] = 0.\n",
    "        A_n[~mask, :] = 0.\n",
    "        A_n[:, ~mask] = 0.\n",
    "    r_c = r_base + r_n\n",
    "    A_c = A_base + A_n\n",
    "    # Ensure self-regulation remains negative\n",
    "    A_c[np.diag_indices(n)] = -np.abs(np.diag(A_c)) - 1e-6\n",
    "    return r_c, A_c\n",
    "\n",
    "\n",
    "print(\"Stable regime builder and hierarchical sampler defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Community Table \u2014 Sparse Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a74  COMMUNITY TABLE \u2014 SPARSE INITIALISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def generate_community_dataframe(\n",
    "    n_species, size_counts, mean_abundance=0.01,\n",
    "    bounds=(0.001, 0.1), sigma_log=1.0, seed=None,\n",
    "    time_value=0.0, comm_name_prefix=\"comm\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate initial communities with sparse composition.\n",
    "    Absent species have abundance EXACTLY 0.0.\n",
    "    Coverage guarantee: all species appear in at least one community.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df   : pd.DataFrame   [Comm_name, Time, sp1..spN]\n",
    "    meta : dict           comm_name -> list of present species indices (0-based)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    mu_log = np.log(mean_abundance) - 0.5 * sigma_log**2\n",
    "    rows, meta = [], {}\n",
    "    counter = 1\n",
    "\n",
    "    for size, count in sorted(size_counts, key=lambda x: x[0]):\n",
    "        size, count = int(size), int(count)\n",
    "        if not (1 <= size <= n_species):\n",
    "            raise ValueError(f\"Community size {size} not in [1, {n_species}]\")\n",
    "\n",
    "        if size == 1:\n",
    "            count = max(count, n_species)  # coverage guarantee\n",
    "            species_seq = list(range(n_species))\n",
    "            if count > n_species:\n",
    "                species_seq += rng.integers(0, n_species, count - n_species).tolist()\n",
    "            for sp_idx in species_seq:\n",
    "                abund = np.zeros(n_species)\n",
    "                abund[sp_idx] = float(np.clip(\n",
    "                    rng.lognormal(mu_log, sigma_log), *bounds\n",
    "                ))\n",
    "                name = f\"{comm_name_prefix}{counter}\"\n",
    "                rows.append([name, time_value] + abund.tolist())\n",
    "                meta[name] = [sp_idx]\n",
    "                counter += 1\n",
    "        else:\n",
    "            for _ in range(count):\n",
    "                present = sorted(\n",
    "                    rng.choice(n_species, size, replace=False).tolist()\n",
    "                )\n",
    "                abund = np.zeros(n_species)\n",
    "                sampled = np.clip(rng.lognormal(mu_log, sigma_log, size), *bounds)\n",
    "                for k, sp in enumerate(present):\n",
    "                    abund[sp] = float(sampled[k])\n",
    "                name = f\"{comm_name_prefix}{counter}\"\n",
    "                rows.append([name, time_value] + abund.tolist())\n",
    "                meta[name] = present\n",
    "                counter += 1\n",
    "\n",
    "    cols = [\"Comm_name\", \"Time\"] + [f\"sp{i+1}\" for i in range(n_species)]\n",
    "    return pd.DataFrame(rows, columns=cols), meta\n",
    "\n",
    "\n",
    "print(\"generate_community_dataframe defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Core gLV Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a75  CORE gLV SIMULATOR (with immigration in ALL RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def _get_sp_cols(df):\n",
    "    return [c for c in df.columns if c.startswith(\"sp\")]\n",
    "\n",
    "\n",
    "def _build_immigration_vec(config, r_base):\n",
    "    \"\"\"\n",
    "    Per-species immigration (propagule rain) rates.\n",
    "    m_i = immigration_rate * |r_i| * immigration_scale\n",
    "\n",
    "    Biology: even absent taxa arrive at low frequency from the environment.\n",
    "    This gives every species a strictly positive influx regardless of\n",
    "    current abundance, fixing the absorbing-zero problem.\n",
    "    \"\"\"\n",
    "    rate  = float(getattr(config, \"immigration_rate\", 1e-4))\n",
    "    scale = float(getattr(config, \"immigration_scale\", 1.0))\n",
    "    m = rate * scale * np.abs(np.asarray(r_base, dtype=float))\n",
    "    return m\n",
    "\n",
    "\n",
    "def _apply_carrying_cap(dxdt, x, config):\n",
    "    \"\"\"\n",
    "    Global carrying capacity: when total biomass approaches K,\n",
    "    apply logistic suppression to all species proportionally.\n",
    "    \"\"\"\n",
    "    if not getattr(config, \"enable_carrying_cap\", False):\n",
    "        return dxdt\n",
    "    K = float(getattr(config, \"carrying_capacity\", 2.0))\n",
    "    total = float(np.sum(np.maximum(x, 0.)))\n",
    "    if total > 0 and K > 0:\n",
    "        suppression = max(0.0, 1.0 - total / K)\n",
    "        # Only suppress positive growth; allow decline\n",
    "        dxdt = np.where(dxdt > 0, dxdt * suppression, dxdt)\n",
    "    return dxdt\n",
    "\n",
    "\n",
    "def _apply_allee(dxdt, x, config):\n",
    "    \"\"\"\n",
    "    Allee effect: species below threshold have reduced growth.\n",
    "    Models cooperative behaviours (quorum sensing, biofilm formation).\n",
    "    \"\"\"\n",
    "    if not getattr(config, \"enable_allee\", False):\n",
    "        return dxdt\n",
    "    threshold = float(getattr(config, \"allee_threshold\", 0.005))\n",
    "    # Smooth Allee: multiply growth by x/(x + threshold)\n",
    "    allee_factor = x / (x + threshold + 1e-15)\n",
    "    # Only modulate positive growth rates\n",
    "    dxdt = np.where(dxdt > 0, dxdt * allee_factor, dxdt)\n",
    "    return dxdt\n",
    "\n",
    "\n",
    "def simulate_gLV_dataframe(df_init, r, A, timepoints,\n",
    "                            species_prefix=\"sp\", atol=1e-8, rtol=1e-6,\n",
    "                            enforce_nonnegative=True,\n",
    "                            antibiotic_fn=None, bloom_fn=None, m=None,\n",
    "                            config=None):\n",
    "    \"\"\"\n",
    "    Simulate gLV for every community in df_init.\n",
    "    dx_i/dt = x_i*(r_i + \u03a3 A_ij x_j) + m_i  [+modifiers]\n",
    "\n",
    "    m : per-species immigration (propagule rain) vector.\n",
    "    \"\"\"\n",
    "    r = np.asarray(r, dtype=float)\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    n = r.shape[0]\n",
    "    sp = [f\"{species_prefix}{i+1}\" for i in range(n)]\n",
    "    t0, tf = float(timepoints[0]), float(timepoints[-1])\n",
    "    m_ = np.zeros(n) if m is None else np.asarray(m, dtype=float)\n",
    "\n",
    "    def rhs(t, x):\n",
    "        xp = np.maximum(np.nan_to_num(x), 0.)\n",
    "        re = r.copy()\n",
    "        if bloom_fn:\n",
    "            re = re + bloom_fn(t)\n",
    "        dxdt = xp * (re + A @ xp) + m_\n",
    "        if antibiotic_fn:\n",
    "            dxdt -= antibiotic_fn(t) * xp\n",
    "        if config is not None:\n",
    "            dxdt = _apply_carrying_cap(dxdt, xp, config)\n",
    "            dxdt = _apply_allee(dxdt, xp, config)\n",
    "        return dxdt\n",
    "\n",
    "    out = []\n",
    "    for comm in df_init[\"Comm_name\"].unique():\n",
    "        df_c = df_init[df_init[\"Comm_name\"] == comm]\n",
    "        init = df_c.sort_values(\"Time\").iloc[0]\n",
    "        x0 = np.maximum(init[sp].to_numpy(dtype=float), 0.)\n",
    "        sol = solve_ivp(rhs, (t0, tf), x0, t_eval=timepoints,\n",
    "                        atol=atol, rtol=rtol, method=\"LSODA\")\n",
    "        if not sol.success:\n",
    "            raise RuntimeError(f\"ODE failed for {comm}: {sol.message}\")\n",
    "        X = np.maximum(sol.y.T, 0.) if enforce_nonnegative else sol.y.T\n",
    "        for i, t in enumerate(timepoints):\n",
    "            out.append([comm, float(t)] + X[i].tolist())\n",
    "\n",
    "    return (pd.DataFrame(out, columns=[\"Comm_name\", \"Time\"] + sp)\n",
    "              .sort_values([\"Comm_name\", \"Time\"]).reset_index(drop=True))\n",
    "\n",
    "\n",
    "def _glv_integrate(r, A, x0, timepoints, atol=1e-8, rtol=1e-6,\n",
    "                   m=None, config=None):\n",
    "    \"\"\"Integrate single community (no obs model). Returns X array (T, S).\"\"\"\n",
    "    m_ = np.zeros(len(r)) if m is None else np.asarray(m, dtype=float)\n",
    "\n",
    "    def rhs(t, x):\n",
    "        x = np.maximum(np.nan_to_num(x), 0.)\n",
    "        dxdt = x * (r + A @ x) + m_\n",
    "        if config is not None:\n",
    "            dxdt = _apply_carrying_cap(dxdt, x, config)\n",
    "            dxdt = _apply_allee(dxdt, x, config)\n",
    "        return dxdt\n",
    "\n",
    "    sol = solve_ivp(rhs, (float(timepoints[0]), float(timepoints[-1])),\n",
    "                    x0, t_eval=timepoints, atol=atol, rtol=rtol, method=\"LSODA\")\n",
    "    if not sol.success:\n",
    "        raise RuntimeError(f\"ODE solver failed: {sol.message}\")\n",
    "    return np.maximum(sol.y.T, 0.)\n",
    "\n",
    "\n",
    "print(\"Core gLV simulator defined (with immigration, carrying cap, Allee).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scenario Registry & Dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a76  SCENARIO REGISTRY & DISPATCHER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "SCENARIO_REGISTRY = {}\n",
    "\n",
    "def register_scenario(name):\n",
    "    def wrapper(func):\n",
    "        SCENARIO_REGISTRY[name] = func\n",
    "        return func\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def simulate_community(config, df_init, timepoints, comm_meta=None, **kwargs):\n",
    "    \"\"\"Master entry: routes to scenario, applies sampling + observation model.\"\"\"\n",
    "    sc = config.scenario\n",
    "    if sc not in SCENARIO_REGISTRY:\n",
    "        raise ValueError(\n",
    "            f\"Scenario '{sc}' not registered. \"\n",
    "            f\"Available: {sorted(SCENARIO_REGISTRY)}\"\n",
    "        )\n",
    "    df_latent = SCENARIO_REGISTRY[sc](\n",
    "        config, df_init=df_init, timepoints=timepoints,\n",
    "        comm_meta=comm_meta, **kwargs\n",
    "    )\n",
    "    df_latent = apply_sampling_design(df_latent, config)\n",
    "    return apply_observation_model(df_latent, config)\n",
    "\n",
    "\n",
    "def list_scenarios():\n",
    "    print(\"Registered scenarios:\", sorted(SCENARIO_REGISTRY))\n",
    "\n",
    "\n",
    "print(\"Dispatcher ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b. Shared Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a76b  SHARED HELPERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "# Store ground-truth parameters for export\n",
    "_GROUND_TRUTH_PARAMS = {}\n",
    "\n",
    "def _build_regime_pair(config, rng=None):\n",
    "    \"\"\"\n",
    "    Returns (r1, A1, r2, A2).\n",
    "    Regime 2 is interpolated: alpha=regime_distance (0=identical, 1=independent).\n",
    "    Also stores parameters in _GROUND_TRUTH_PARAMS for export.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(config.seed) if rng is None else rng\n",
    "    n   = config.n_species\n",
    "    kw1 = getattr(config, \"A_structure_kwargs\",\n",
    "                   dict(p=0.15, offdiag_std=0.03, diag_range=(-0.6, -0.2)))\n",
    "    kw2 = getattr(config, \"A_structure_kwargs2\", None) or kw1\n",
    "    st  = getattr(config, \"A_structure\", \"sparse\")\n",
    "    sm  = getattr(config, \"stability_margin\", 0.03)\n",
    "\n",
    "    r1, A1, _ = construct_stable_regime(\n",
    "        n, structure=st, stability_margin=sm, rng=rng,\n",
    "        structure_kwargs=kw1\n",
    "    )\n",
    "    r2r, A2r, _ = construct_stable_regime(\n",
    "        n, structure=st, stability_margin=sm,\n",
    "        rng=np.random.default_rng(config.seed + 10_000),\n",
    "        structure_kwargs=kw2\n",
    "    )\n",
    "    alpha = float(getattr(config, \"regime_distance\", 1.0))\n",
    "    r2 = (1 - alpha) * r1 + alpha * r2r\n",
    "    A2 = (1 - alpha) * A1 + alpha * A2r\n",
    "\n",
    "    # Store for export (FIX #12: ground-truth regime parameters)\n",
    "    _GROUND_TRUTH_PARAMS.update({\n",
    "        \"r1\": r1.tolist(), \"A1\": A1.tolist(),\n",
    "        \"r2\": r2.tolist(), \"A2\": A2.tolist(),\n",
    "    })\n",
    "\n",
    "    return r1, A1, r2, A2\n",
    "\n",
    "\n",
    "def _build_antibiotic_fn(config, n_species):\n",
    "    if not getattr(config, \"enable_antibiotic\", False):\n",
    "        return None\n",
    "    t0 = float(getattr(config, \"antibiotic_start\", 8.))\n",
    "    dur = float(getattr(config, \"antibiotic_duration\", 3.))\n",
    "    k = getattr(config, \"antibiotic_kill_rates\", None)\n",
    "    k = np.full(n_species, 0.5) if k is None else np.asarray(k, float)\n",
    "    return lambda t: k if (t0 <= t <= t0 + dur) else np.zeros(n_species)\n",
    "\n",
    "\n",
    "def _build_bloom_fn(config, n_species):\n",
    "    if not getattr(config, \"enable_bloom\", False):\n",
    "        return None\n",
    "    t0 = float(getattr(config, \"bloom_start\", 5.))\n",
    "    dur = float(getattr(config, \"bloom_duration\", 2.))\n",
    "    sp = int(getattr(config, \"bloom_species\", 0))\n",
    "    boost = float(getattr(config, \"bloom_boost\", 2.))\n",
    "    delta = np.zeros(n_species)\n",
    "    delta[sp] = boost\n",
    "    return lambda t: delta if (t0 <= t <= t0 + dur) else np.zeros(n_species)\n",
    "\n",
    "\n",
    "def _get_community_trigger_params(config, comm_name):\n",
    "    \"\"\"\n",
    "    FIX #10: Per-community heterogeneity for hidden/cumulative triggers.\n",
    "    Jitters theta and k_u per community using comm_name as seed source.\n",
    "    \"\"\"\n",
    "    theta_base = float(getattr(config, \"theta\", 0.5))\n",
    "    k_u_base = float(getattr(config, \"k_u\", 0.4))\n",
    "    sig_theta = float(getattr(config, \"trigger_sigma_theta\", 0.0))\n",
    "    sig_k_u = float(getattr(config, \"trigger_sigma_k_u\", 0.0))\n",
    "\n",
    "    if sig_theta == 0.0 and sig_k_u == 0.0:\n",
    "        return theta_base, k_u_base\n",
    "\n",
    "    rng = make_rng(f\"trigger_{comm_name}\", config.seed)\n",
    "    theta_c = float(np.clip(rng.normal(theta_base, sig_theta), 0.05, 0.95))\n",
    "    k_u_c = float(np.clip(rng.normal(k_u_base, sig_k_u), 0.05, 2.0))\n",
    "    return theta_c, k_u_c\n",
    "\n",
    "\n",
    "print(\"Shared helpers defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scenarios \u2014 Baseline gLV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a77  SCENARIOS \u2014 BASELINE gLV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "@register_scenario(\"baseline_glv\")\n",
    "def run_baseline_glv(config, df_init, timepoints, comm_meta=None, **kwargs):\n",
    "    return simulate_gLV_dataframe(df_init=df_init, timepoints=timepoints,\n",
    "                                   config=config, **kwargs)\n",
    "\n",
    "\n",
    "@register_scenario(\"baseline_glv_stable\")\n",
    "def run_baseline_glv_stable(config, df_init, timepoints, comm_meta=None, **kwargs):\n",
    "    rng = np.random.default_rng(config.seed)\n",
    "    n = config.n_species\n",
    "    r_base, A_base, x_star = construct_stable_regime(\n",
    "        n, structure=getattr(config, \"A_structure\", \"sparse\"),\n",
    "        stability_margin=getattr(config, \"stability_margin\", 0.03), rng=rng,\n",
    "        structure_kwargs=getattr(config, \"A_structure_kwargs\",\n",
    "                                  dict(p=0.15, offdiag_std=0.03,\n",
    "                                       diag_range=(-0.6, -0.2))))\n",
    "\n",
    "    _GROUND_TRUTH_PARAMS.update({\n",
    "        \"r1\": r_base.tolist(), \"A1\": A_base.tolist(),\n",
    "        \"x_star\": x_star.tolist(),\n",
    "    })\n",
    "\n",
    "    hier = getattr(config, \"enable_hierarchical\", True)\n",
    "    sig_r = getattr(config, \"sigma_r\", 0.15)\n",
    "    sig_A = getattr(config, \"sigma_A\", 0.05)\n",
    "    ab_fn = _build_antibiotic_fn(config, n)\n",
    "    bl_fn = _build_bloom_fn(config, n)\n",
    "\n",
    "    out = []\n",
    "    for comm in sorted(df_init[\"Comm_name\"].unique()):\n",
    "        df_c = df_init[df_init[\"Comm_name\"] == comm].copy()\n",
    "        pm = None\n",
    "        if comm_meta and comm in comm_meta:\n",
    "            pm = np.zeros(n, dtype=bool)\n",
    "            pm[comm_meta[comm]] = True\n",
    "        r_c, A_c = (\n",
    "            sample_hierarchical_community_params(\n",
    "                r_base, A_base, sig_r, sig_A, rng, pm)\n",
    "            if hier else (r_base.copy(), A_base.copy())\n",
    "        )\n",
    "        m = _build_immigration_vec(config, r_c)\n",
    "        out.append(simulate_gLV_dataframe(\n",
    "            df_c, r_c, A_c, timepoints,\n",
    "            antibiotic_fn=ab_fn, bloom_fn=bl_fn, m=m, config=config\n",
    "        ))\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"Baseline scenarios registered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scenarios \u2014 Regime Switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a78  SCENARIOS \u2014 REGIME SWITCHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 Soft-switch utilities \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "def _soft_weight(t, t_switch, epsilon):\n",
    "    return 1. / (1. + np.exp(-(t - t_switch) / epsilon))\n",
    "\n",
    "\n",
    "def _soft_rhs(t, x, r1, A1, r2, A2, t_switch, epsilon, m, config=None):\n",
    "    x = np.maximum(x, 0.)\n",
    "    w = _soft_weight(t, t_switch, epsilon)\n",
    "    dxdt = (1 - w) * x * (r1 + A1 @ x) + w * x * (r2 + A2 @ x) + m\n",
    "    if config is not None:\n",
    "        dxdt = _apply_carrying_cap(dxdt, x, config)\n",
    "        dxdt = _apply_allee(dxdt, x, config)\n",
    "    return dxdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 TIME_SWITCH \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "@register_scenario(\"time_switch\")\n",
    "def run_time_switch(config, df_init, timepoints, comm_meta=None, **kwargs):\n",
    "    \"\"\"Hard switch at t_switch. Supports per-community jitter.\"\"\"\n",
    "    rng = np.random.default_rng(config.seed)\n",
    "    r1, A1, r2, A2 = _build_regime_pair(config, rng)\n",
    "    t0, tf = float(timepoints[0]), float(timepoints[-1])\n",
    "    t_sw_global = float(getattr(config, \"t_switch\", 10.))\n",
    "    per_comm = bool(getattr(config, \"per_comm_switch\", False))\n",
    "    spread = float(getattr(config, \"switch_spread\", 3.))\n",
    "    m = _build_immigration_vec(config, r1)\n",
    "    out = []\n",
    "\n",
    "    for comm in sorted(df_init[\"Comm_name\"].unique()):\n",
    "        df_c = df_init[df_init[\"Comm_name\"] == comm].copy()\n",
    "        if per_comm:\n",
    "            t_sw = float(np.clip(\n",
    "                make_rng(f\"sw_{comm}\", config.seed).normal(t_sw_global, spread),\n",
    "                t0 + 1e-3, tf - 1e-3\n",
    "            ))\n",
    "        else:\n",
    "            t_sw = t_sw_global\n",
    "\n",
    "        tp1 = timepoints[timepoints <= t_sw]\n",
    "        if len(tp1) == 0 or tp1[-1] != t_sw:\n",
    "            tp1 = np.sort(np.unique(np.append(tp1, t_sw)))\n",
    "        tp2 = timepoints[timepoints >= t_sw]\n",
    "        if len(tp2) == 0 or tp2[0] != t_sw:\n",
    "            tp2 = np.sort(np.unique(np.append(t_sw, tp2)))\n",
    "\n",
    "        seg1 = simulate_gLV_dataframe(df_c, r1, A1, tp1, m=m, config=config)\n",
    "        sp = _get_sp_cols(seg1)\n",
    "        init2 = seg1[np.isclose(seg1[\"Time\"].astype(float), t_sw)][\n",
    "            [\"Comm_name\", \"Time\"] + sp\n",
    "        ].copy()\n",
    "        init2[\"Time\"] = t_sw\n",
    "        seg2 = simulate_gLV_dataframe(init2, r2, A2, tp2, m=m, config=config)\n",
    "\n",
    "        df_out = pd.concat([\n",
    "            seg1,\n",
    "            seg2[~np.isclose(seg2[\"Time\"].astype(float), t_sw)]\n",
    "        ], ignore_index=True)\n",
    "        df_out[\"regime\"] = (df_out[\"Time\"] > t_sw).astype(int)\n",
    "        df_out[\"t_switch_true\"] = t_sw\n",
    "        out.append(df_out)\n",
    "\n",
    "    return pd.concat(out, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 SOFT_SWITCH \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "@register_scenario(\"soft_switch\")\n",
    "def run_soft_switch(config, df_init, timepoints, comm_meta=None, **kwargs):\n",
    "    \"\"\"Logistic interpolation between two regimes.\"\"\"\n",
    "    rng = np.random.default_rng(config.seed)\n",
    "    r1, A1, r2, A2 = _build_regime_pair(config, rng)\n",
    "    t_sw = float(getattr(config, \"t_switch\", 10.))\n",
    "    eps = float(getattr(config, \"epsilon\", 1.))\n",
    "    m = _build_immigration_vec(config, r1)\n",
    "    sp = _get_sp_cols(df_init)\n",
    "    out = []\n",
    "\n",
    "    for comm, sub in df_init.groupby(\"Comm_name\"):\n",
    "        x0 = np.maximum(sub[sp].iloc[0].values.astype(float), 0.)\n",
    "        sol = solve_ivp(\n",
    "            lambda t, x: _soft_rhs(t, x, r1, A1, r2, A2, t_sw, eps, m, config),\n",
    "            (timepoints[0], timepoints[-1]), x0,\n",
    "            t_eval=timepoints, rtol=1e-6, atol=1e-9, method=\"LSODA\"\n",
    "        )\n",
    "        if not sol.success:\n",
    "            raise RuntimeError(f\"soft_switch failed for {comm}: {sol.message}\")\n",
    "        df_c = pd.DataFrame(np.maximum(sol.y.T, 0.), columns=sp)\n",
    "        df_c[\"Time\"] = sol.t\n",
    "        df_c[\"Comm_name\"] = comm\n",
    "        df_c[\"w\"] = _soft_weight(sol.t, t_sw, eps)\n",
    "        df_c[\"regime\"] = (df_c[\"w\"] > 0.5).astype(int)\n",
    "        out.append(df_c)\n",
    "\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"time_switch and soft_switch registered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 HIDDEN TRIGGER \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "def _glv_u_rhs(t, y, r, A, k_u, m, config=None):\n",
    "    n = len(r)\n",
    "    x = np.maximum(y[:n], 0.)\n",
    "    u = y[n]\n",
    "    dxdt = x * (r + A @ x) + m\n",
    "    if config is not None:\n",
    "        dxdt = _apply_carrying_cap(dxdt, x, config)\n",
    "        dxdt = _apply_allee(dxdt, x, config)\n",
    "    dudt = k_u * (1. - u)\n",
    "    return np.concatenate([dxdt, [dudt]])\n",
    "\n",
    "\n",
    "def _event_u(theta):\n",
    "    e = lambda t, y: y[-1] - theta\n",
    "    e.terminal = True\n",
    "    e.direction = 1\n",
    "    return e\n",
    "\n",
    "\n",
    "def _w_from_u(u, theta, epsilon):\n",
    "    return 1. / (1. + np.exp(-(u - theta) / epsilon))\n",
    "\n",
    "\n",
    "@register_scenario(\"hidden_trigger\")\n",
    "def run_hidden_trigger(config, df_init, timepoints, comm_meta=None, **kwargs):\n",
    "    rng = np.random.default_rng(config.seed)\n",
    "    r1, A1, r2, A2 = _build_regime_pair(config, rng)\n",
    "    eps = float(getattr(config, \"epsilon\", 0.1))\n",
    "    u0_ = float(getattr(config, \"u0\", 0.))\n",
    "    m = _build_immigration_vec(config, r1)\n",
    "    sp = _get_sp_cols(df_init)\n",
    "    t0, tf = float(timepoints[0]), float(timepoints[-1])\n",
    "    out = []\n",
    "\n",
    "    for comm, sub in df_init.groupby(\"Comm_name\"):\n",
    "        # FIX #10: per-community trigger heterogeneity\n",
    "        theta_c, k_u_c = _get_community_trigger_params(config, comm)\n",
    "\n",
    "        x0 = np.maximum(sub[sp].iloc[0].values.astype(float), 0.)\n",
    "        y0 = np.concatenate([x0, [u0_]])\n",
    "\n",
    "        sol1 = solve_ivp(\n",
    "            lambda t, y: _glv_u_rhs(t, y, r1, A1, k_u_c, m, config),\n",
    "            (t0, tf), y0,\n",
    "            events=_event_u(theta_c), dense_output=True,\n",
    "            rtol=1e-6, atol=1e-9, method=\"LSODA\"\n",
    "        )\n",
    "        t_sw = (float(sol1.t_events[0][0])\n",
    "                if sol1.t_events[0].size > 0 else tf)\n",
    "\n",
    "        tp1 = timepoints[timepoints <= t_sw]\n",
    "        if not len(tp1) or tp1[-1] != t_sw:\n",
    "            tp1 = np.sort(np.unique(np.append(tp1, t_sw)))\n",
    "        Y1 = sol1.sol(tp1).T\n",
    "        n_sp = len(sp)\n",
    "        df1 = pd.DataFrame(np.maximum(Y1[:, :n_sp], 0.), columns=sp)\n",
    "        df1[\"u\"] = Y1[:, n_sp]\n",
    "        df1[\"w\"] = _w_from_u(Y1[:, n_sp], theta_c, eps)\n",
    "        df1[\"regime\"] = 0\n",
    "        df1[\"Time\"] = tp1\n",
    "        df1[\"Comm_name\"] = comm\n",
    "\n",
    "        if t_sw >= tf:\n",
    "            out.append(df1[df1[\"Time\"] <= tf])\n",
    "            continue\n",
    "\n",
    "        y_sw = sol1.sol(t_sw).reshape(-1)\n",
    "        sol2 = solve_ivp(\n",
    "            lambda t, y: _glv_u_rhs(t, y, r2, A2, k_u_c, m, config),\n",
    "            (t_sw, tf), y_sw,\n",
    "            dense_output=True, rtol=1e-6, atol=1e-9, method=\"LSODA\"\n",
    "        )\n",
    "        tp2 = timepoints[timepoints >= t_sw]\n",
    "        if not len(tp2) or tp2[0] != t_sw:\n",
    "            tp2 = np.sort(np.unique(np.append(t_sw, tp2)))\n",
    "        Y2 = sol2.sol(tp2).T\n",
    "        df2 = pd.DataFrame(np.maximum(Y2[:, :n_sp], 0.), columns=sp)\n",
    "        df2[\"u\"] = Y2[:, n_sp]\n",
    "        df2[\"w\"] = _w_from_u(Y2[:, n_sp], theta_c, eps)\n",
    "        df2[\"regime\"] = 1\n",
    "        df2[\"Time\"] = tp2\n",
    "        df2[\"Comm_name\"] = comm\n",
    "        df2 = df2[df2[\"Time\"] != t_sw].copy()\n",
    "\n",
    "        df_out = pd.concat([df1, df2], ignore_index=True)\n",
    "        df_out[\"t_switch_true\"] = t_sw\n",
    "        out.append(df_out)\n",
    "\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"hidden_trigger registered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 CUMULATIVE TRIGGER \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "def _glv_integral_rhs(t, y, r, A, idx_M, m, config=None):\n",
    "    n = len(r)\n",
    "    x = np.maximum(y[:n], 0.)\n",
    "    dxdt = x * (r + A @ x) + m\n",
    "    if config is not None:\n",
    "        dxdt = _apply_carrying_cap(dxdt, x, config)\n",
    "        dxdt = _apply_allee(dxdt, x, config)\n",
    "    da = float(np.sum(x[idx_M]))\n",
    "    return np.concatenate([dxdt, [da]])\n",
    "\n",
    "\n",
    "def _event_integral(c1, M_init):\n",
    "    e = lambda t, y: c1 * y[-1] - M_init\n",
    "    e.terminal = True\n",
    "    e.direction = 1\n",
    "    return e\n",
    "\n",
    "\n",
    "def _w_from_integral(a, c1, M_init, epsilon):\n",
    "    return 1. / (1. + np.exp(-(c1 * a - M_init) / epsilon))\n",
    "\n",
    "\n",
    "@register_scenario(\"cumulative_trigger\")\n",
    "def run_cumulative_trigger(config, df_init, timepoints, comm_meta=None, **kwargs):\n",
    "    rng = np.random.default_rng(config.seed)\n",
    "    r1, A1, r2, A2 = _build_regime_pair(config, rng)\n",
    "\n",
    "    c1_val = float(getattr(config, \"c1\", 1.))\n",
    "    M_init = float(getattr(config, \"M_init\", 0.05))\n",
    "    eps = float(getattr(config, \"epsilon\", 0.01))\n",
    "    idx_M = getattr(config, \"idx_M\", None)\n",
    "    idx_M = (np.arange(config.n_species, dtype=int)\n",
    "             if idx_M is None else np.array(list(idx_M), dtype=int))\n",
    "    a0_ = float(getattr(config, \"a0\", 0.))\n",
    "    m = _build_immigration_vec(config, r1)\n",
    "    sp = _get_sp_cols(df_init)\n",
    "    t0, tf = float(timepoints[0]), float(timepoints[-1])\n",
    "    ev = _event_integral(c1_val, M_init)\n",
    "    out = []\n",
    "\n",
    "    for comm, sub in df_init.groupby(\"Comm_name\"):\n",
    "        x0 = np.maximum(sub[sp].iloc[0].values.astype(float), 0.)\n",
    "        y0 = np.concatenate([x0, [a0_]])\n",
    "\n",
    "        sol1 = solve_ivp(\n",
    "            lambda t, y: _glv_integral_rhs(t, y, r1, A1, idx_M, m, config),\n",
    "            (t0, tf), y0, events=ev, dense_output=True,\n",
    "            rtol=1e-6, atol=1e-9, method=\"LSODA\"\n",
    "        )\n",
    "        t_sw = (float(sol1.t_events[0][0])\n",
    "                if sol1.t_events[0].size > 0 else tf)\n",
    "\n",
    "        tp1 = timepoints[timepoints <= t_sw]\n",
    "        if not len(tp1) or tp1[-1] != t_sw:\n",
    "            tp1 = np.sort(np.unique(np.append(tp1, t_sw)))\n",
    "        Y1 = sol1.sol(tp1).T\n",
    "        n_sp = len(sp)\n",
    "        df1 = pd.DataFrame(np.maximum(Y1[:, :n_sp], 0.), columns=sp)\n",
    "        df1[\"A_M\"] = Y1[:, -1]\n",
    "        df1[\"w\"] = _w_from_integral(Y1[:, -1], c1_val, M_init, eps)\n",
    "        df1[\"regime\"] = 0\n",
    "        df1[\"Time\"] = tp1\n",
    "        df1[\"Comm_name\"] = comm\n",
    "\n",
    "        if t_sw >= tf:\n",
    "            out.append(df1[df1[\"Time\"] <= tf])\n",
    "            continue\n",
    "\n",
    "        y_sw = sol1.sol(t_sw).reshape(-1)\n",
    "        sol2 = solve_ivp(\n",
    "            lambda t, y: _glv_integral_rhs(t, y, r2, A2, idx_M, m, config),\n",
    "            (t_sw, tf), y_sw, dense_output=True,\n",
    "            rtol=1e-6, atol=1e-9, method=\"LSODA\"\n",
    "        )\n",
    "        tp2 = timepoints[timepoints >= t_sw]\n",
    "        if not len(tp2) or tp2[0] != t_sw:\n",
    "            tp2 = np.sort(np.unique(np.append(t_sw, tp2)))\n",
    "        Y2 = sol2.sol(tp2).T\n",
    "        df2 = pd.DataFrame(np.maximum(Y2[:, :n_sp], 0.), columns=sp)\n",
    "        df2[\"A_M\"] = Y2[:, -1]\n",
    "        df2[\"w\"] = _w_from_integral(Y2[:, -1], c1_val, M_init, eps)\n",
    "        df2[\"regime\"] = 1\n",
    "        df2[\"Time\"] = tp2\n",
    "        df2[\"Comm_name\"] = comm\n",
    "        df2 = df2[df2[\"Time\"] != t_sw].copy()\n",
    "\n",
    "        df_out = pd.concat([df1, df2], ignore_index=True)\n",
    "        df_out[\"t_switch_true\"] = t_sw\n",
    "        out.append(df_out)\n",
    "\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"cumulative_trigger registered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Scenarios \u2014 Resource / Cross-Feeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a79  SCENARIOS \u2014 RESOURCE / CROSS-FEEDING DYNAMICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def build_inflow_fn(config, base_inflow):\n",
    "    base_inflow = np.asarray(base_inflow, dtype=float)\n",
    "    if not getattr(config, \"enable_forcing\", False):\n",
    "        return lambda t: base_inflow.copy()\n",
    "    shocks = getattr(config, \"diet_shocks\", None) or []\n",
    "    parsed = [(float(s[\"start\"]), float(s[\"end\"]),\n",
    "               np.asarray(s[\"delta\"], float)) for s in shocks]\n",
    "    def fn(t):\n",
    "        v = base_inflow.copy()\n",
    "        for s, e, d in parsed:\n",
    "            if s <= t <= e:\n",
    "                v = v + d\n",
    "        return v\n",
    "    return fn\n",
    "\n",
    "\n",
    "def simulate_resource_coupled_dataframe(\n",
    "    df_init, r, A, timepoints, R0, B, consume, inflow,\n",
    "    delta_R, K_half, config,\n",
    "    produce=None, clip_nonneg=True, solver_method=\"LSODA\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Species + resource ODE:\n",
    "      dx_i/dt = x_i*(r_i + Ax + B@R) + m_i\n",
    "      dR_k/dt = inflow_k - delta_R_k*R_k\n",
    "                - sum_i consume_{ik}*x_i*(R_k/(K_k+R_k))\n",
    "                [+ sum_i produce_{ik}*x_i]\n",
    "    \"\"\"\n",
    "    sp = _get_sp_cols(df_init)\n",
    "    x0 = df_init.loc[\n",
    "        df_init[\"Time\"] == df_init[\"Time\"].min(), sp\n",
    "    ].iloc[0].values.astype(float)\n",
    "    S, K = len(x0), len(R0)\n",
    "    r = np.asarray(r, float).reshape(S)\n",
    "    A = np.asarray(A, float).reshape(S, S)\n",
    "    R0 = np.asarray(R0, float).reshape(K)\n",
    "    B = np.asarray(B, float).reshape(S, K)\n",
    "    consume = np.asarray(consume, float).reshape(S, K)\n",
    "    delta_R = np.asarray(delta_R, float).reshape(K)\n",
    "    K_half = np.asarray(K_half, float).reshape(K)\n",
    "    prod_ = (np.zeros((S, K)) if produce is None\n",
    "             else np.asarray(produce, float).reshape(S, K))\n",
    "    inflow_t = build_inflow_fn(config, np.asarray(inflow, float).reshape(K))\n",
    "    y0 = np.concatenate([x0, R0])\n",
    "    m_res = _build_immigration_vec(config, r)\n",
    "\n",
    "    def rhs(t, y):\n",
    "        x = np.maximum(y[:S], 0.) if clip_nonneg else y[:S]\n",
    "        R = np.maximum(y[S:], 0.) if clip_nonneg else y[S:]\n",
    "        dx = x * (r + A @ x + B @ R) + m_res\n",
    "        dx = _apply_carrying_cap(dx, x, config)\n",
    "        dx = _apply_allee(dx, x, config)\n",
    "        uptake = (x[:, None] * consume) * (R / (K_half + R + 1e-12))[None, :]\n",
    "        dR = (inflow_t(t) - delta_R * R\n",
    "              - uptake.sum(0) + (x[:, None] * prod_).sum(0))\n",
    "        return np.concatenate([dx, dR])\n",
    "\n",
    "    sol = solve_ivp(rhs, (timepoints.min(), timepoints.max()), y0,\n",
    "                    t_eval=timepoints, method=solver_method)\n",
    "    if not sol.success:\n",
    "        raise RuntimeError(f\"Resource ODE failed: {sol.message}\")\n",
    "    X, Rmat = sol.y.T[:, :S], sol.y.T[:, S:]\n",
    "    df_out = pd.DataFrame({\n",
    "        \"Comm_name\": df_init[\"Comm_name\"].iloc[0], \"Time\": sol.t\n",
    "    })\n",
    "    for i in range(S):\n",
    "        df_out[f\"sp{i+1}\"] = np.maximum(X[:, i], 0.)\n",
    "    if getattr(config, \"export_resources\", False):\n",
    "        for k in range(K):\n",
    "            df_out[f\"R{k+1}\"] = np.maximum(Rmat[:, k], 0.)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def _run_resource_scenario(config, df_init, timepoints, crossfeeding):\n",
    "    rng = np.random.default_rng(getattr(config, \"seed\", 42))\n",
    "    n = config.n_species\n",
    "    K = int(getattr(config, \"n_metabolites\", 1))\n",
    "    r_base, A_base, _ = construct_stable_regime(\n",
    "        n, structure=getattr(config, \"A_structure\", \"sparse\"),\n",
    "        stability_margin=getattr(config, \"stability_margin\", 0.03), rng=rng,\n",
    "        structure_kwargs=getattr(config, \"A_structure_kwargs\",\n",
    "                                  dict(p=0.15, offdiag_std=0.03,\n",
    "                                       diag_range=(-0.6, -0.2))))\n",
    "    hier = getattr(config, \"enable_hierarchical\", True)\n",
    "    sig_r = getattr(config, \"sigma_r\", 0.15)\n",
    "    sig_A = getattr(config, \"sigma_A\", 0.05)\n",
    "    R0 = np.asarray(getattr(config, \"R0\", np.ones(K)), float).reshape(K)\n",
    "    B = np.asarray(getattr(config, \"B\", rng.normal(0, .2, (n, K))),\n",
    "                   float).reshape(n, K)\n",
    "    consume = np.asarray(\n",
    "        getattr(config, \"consume\", np.abs(rng.normal(0, .2, (n, K)))),\n",
    "        float).reshape(n, K)\n",
    "    inflow = np.asarray(\n",
    "        getattr(config, \"inflow\", np.ones(K) * .05), float).reshape(K)\n",
    "    delta_R = np.asarray(\n",
    "        getattr(config, \"delta_R\", np.ones(K) * .1), float).reshape(K)\n",
    "    K_half = np.asarray(\n",
    "        getattr(config, \"K_half\", np.ones(K) * .5), float).reshape(K)\n",
    "    produce = None\n",
    "    if crossfeeding:\n",
    "        sc = float(getattr(config, \"produce_scale\", 0.05))\n",
    "        produce = np.asarray(\n",
    "            getattr(config, \"produce\",\n",
    "                    np.abs(rng.normal(0, 1, (n, K))) * sc),\n",
    "            float).reshape(n, K)\n",
    "    solver = getattr(config, \"resource_solver_method\", \"LSODA\")\n",
    "    clip_nn = bool(getattr(config, \"clip_nonneg\", True))\n",
    "\n",
    "    out = []\n",
    "    for comm in sorted(df_init[\"Comm_name\"].unique()):\n",
    "        df_c = df_init[df_init[\"Comm_name\"] == comm].copy()\n",
    "        r_c, A_c = (\n",
    "            sample_hierarchical_community_params(\n",
    "                r_base, A_base, sig_r, sig_A, rng)\n",
    "            if hier else (r_base, A_base)\n",
    "        )\n",
    "        out.append(simulate_resource_coupled_dataframe(\n",
    "            df_c, r_c, A_c, timepoints, R0, B, consume, inflow,\n",
    "            delta_R, K_half, config,\n",
    "            produce=produce, clip_nonneg=clip_nn, solver_method=solver\n",
    "        ))\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "@register_scenario(\"resource_coupled\")\n",
    "def run_resource_coupled(config, df_init, timepoints, comm_meta=None, **kwargs):\n",
    "    return _run_resource_scenario(config, df_init, timepoints, crossfeeding=False)\n",
    "\n",
    "\n",
    "@register_scenario(\"resource_crossfeeding\")\n",
    "def run_resource_crossfeeding(config, df_init, timepoints,\n",
    "                               comm_meta=None, **kwargs):\n",
    "    return _run_resource_scenario(config, df_init, timepoints, crossfeeding=True)\n",
    "\n",
    "\n",
    "print(\"resource_coupled and resource_crossfeeding registered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Scenarios \u2014 Bistability, Dormancy, Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a710  SCENARIOS \u2014 BISTABILITY, DORMANCY, ANTIBIOTIC, DRIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "@register_scenario(\"bistable\")\n",
    "def run_bistable(config, df_init, timepoints, comm_meta=None, **kwargs):\n",
    "    \"\"\"Priority-effects bistability: two guilds compete for exclusion.\"\"\"\n",
    "    rng = np.random.default_rng(config.seed)\n",
    "    n = config.n_species\n",
    "    half = n // 2\n",
    "    grp1 = list(range(half))\n",
    "    grp2 = list(range(half, n))\n",
    "    comp = float(getattr(config, \"bistable_comp_strength\", 0.25))\n",
    "    fac = float(getattr(config, \"bistable_fac_strength\", 0.02))\n",
    "\n",
    "    A = np.zeros((n, n))\n",
    "    A[np.diag_indices(n)] = rng.uniform(-0.6, -0.3, n)\n",
    "    for i in grp1:\n",
    "        for j in grp2:\n",
    "            A[i, j] = -comp * (1 + rng.normal(0, .05))\n",
    "            A[j, i] = -comp * (1 + rng.normal(0, .05))\n",
    "    for g in [grp1, grp2]:\n",
    "        for i in g:\n",
    "            for j in g:\n",
    "                if i != j and rng.random() < 0.4:\n",
    "                    A[i, j] = fac * rng.normal(1, .1)\n",
    "\n",
    "    x_eq1 = np.zeros(n)\n",
    "    x_eq1[grp1] = rng.lognormal(-3.5, .5, half)\n",
    "    x_eq2 = np.zeros(n)\n",
    "    x_eq2[grp2] = rng.lognormal(-3.5, .5, n - half)\n",
    "    r = -(A @ (0.5 * (x_eq1 + x_eq2)))\n",
    "\n",
    "    m = _build_immigration_vec(config, r)\n",
    "    out = []\n",
    "    for comm in sorted(df_init[\"Comm_name\"].unique()):\n",
    "        df_c = df_init[df_init[\"Comm_name\"] == comm].copy()\n",
    "        sp = _get_sp_cols(df_c)\n",
    "        x0 = df_c[sp].iloc[0].values.astype(float)\n",
    "        basin = 1 if x0[grp1].sum() >= x0[grp2].sum() else 2\n",
    "        df_s = simulate_gLV_dataframe(df_c, r, A, timepoints, m=m, config=config)\n",
    "        df_s[\"basin\"] = basin\n",
    "        out.append(df_s)\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "@register_scenario(\"dormancy\")\n",
    "def run_dormancy(config, df_init, timepoints, comm_meta=None, **kwargs):\n",
    "    \"\"\"Active <-> dormant transitions (stress-triggered).\"\"\"\n",
    "    rng = np.random.default_rng(config.seed)\n",
    "    n = config.n_species\n",
    "    r, A, _ = construct_stable_regime(\n",
    "        n, structure=getattr(config, \"A_structure\", \"sparse\"),\n",
    "        stability_margin=getattr(config, \"stability_margin\", 0.03), rng=rng,\n",
    "        structure_kwargs=getattr(config, \"A_structure_kwargs\",\n",
    "                                  dict(p=0.15, offdiag_std=0.03,\n",
    "                                       diag_range=(-0.6, -0.2))))\n",
    "    gamma = float(getattr(config, \"dormancy_rate\", 0.05))\n",
    "    delta = float(getattr(config, \"revival_rate\", 0.10))\n",
    "    thr = float(getattr(config, \"dormancy_threshold\", 0.01))\n",
    "    sp = _get_sp_cols(df_init)\n",
    "    m_dorm = _build_immigration_vec(config, r)\n",
    "    out = []\n",
    "\n",
    "    for comm, sub in df_init.groupby(\"Comm_name\"):\n",
    "        x0 = np.maximum(sub[sp].iloc[0].values.astype(float), 0.)\n",
    "        y0 = np.concatenate([x0, np.zeros(n)])\n",
    "\n",
    "        def rhs(t, y, _r=r, _A=A, _n=n):\n",
    "            x = np.maximum(y[:_n], 0.)\n",
    "            d = np.maximum(y[_n:], 0.)\n",
    "            sl = (x < thr).astype(float)\n",
    "            dx = x * (_r + _A @ x) - gamma * x * sl + delta * d + m_dorm\n",
    "            dd = gamma * x * sl - delta * d\n",
    "            return np.concatenate([dx, dd])\n",
    "\n",
    "        sol = solve_ivp(rhs, (timepoints[0], timepoints[-1]), y0,\n",
    "                        t_eval=timepoints, rtol=1e-6, atol=1e-9,\n",
    "                        method=\"LSODA\")\n",
    "        if not sol.success:\n",
    "            raise RuntimeError(f\"dormancy ODE failed for {comm}\")\n",
    "        X = np.maximum(sol.y[:n].T, 0.)\n",
    "        D = np.maximum(sol.y[n:].T, 0.)\n",
    "        df_c = pd.DataFrame(X, columns=sp)\n",
    "        for i in range(n):\n",
    "            df_c[f\"d{i+1}\"] = D[:, i]\n",
    "        df_c[\"Time\"] = sol.t\n",
    "        df_c[\"Comm_name\"] = comm\n",
    "        out.append(df_c)\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "@register_scenario(\"antibiotic_pulse\")\n",
    "def run_antibiotic_pulse(config, df_init, timepoints,\n",
    "                          comm_meta=None, **kwargs):\n",
    "    \"\"\"Stable gLV + species-specific antibiotic kill.\"\"\"\n",
    "    rng = np.random.default_rng(config.seed)\n",
    "    n = config.n_species\n",
    "    r_base, A_base, _ = construct_stable_regime(\n",
    "        n, structure=getattr(config, \"A_structure\", \"sparse\"),\n",
    "        stability_margin=getattr(config, \"stability_margin\", 0.03), rng=rng,\n",
    "        structure_kwargs=getattr(config, \"A_structure_kwargs\",\n",
    "                                  dict(p=0.15, offdiag_std=0.03,\n",
    "                                       diag_range=(-0.6, -0.2))))\n",
    "    hier = getattr(config, \"enable_hierarchical\", True)\n",
    "    sig_r = getattr(config, \"sigma_r\", 0.15)\n",
    "    sig_A = getattr(config, \"sigma_A\", 0.05)\n",
    "    ab_fn = _build_antibiotic_fn(config, n)\n",
    "    out = []\n",
    "    for comm in sorted(df_init[\"Comm_name\"].unique()):\n",
    "        df_c = df_init[df_init[\"Comm_name\"] == comm].copy()\n",
    "        r_c, A_c = (\n",
    "            sample_hierarchical_community_params(\n",
    "                r_base, A_base, sig_r, sig_A, rng)\n",
    "            if hier else (r_base, A_base)\n",
    "        )\n",
    "        m = _build_immigration_vec(config, r_c)\n",
    "        df_s = simulate_gLV_dataframe(\n",
    "            df_c, r_c, A_c, timepoints,\n",
    "            antibiotic_fn=ab_fn, m=m, config=config\n",
    "        )\n",
    "        t0_ab = float(getattr(config, \"antibiotic_start\", 8.))\n",
    "        dur_ab = float(getattr(config, \"antibiotic_duration\", 3.))\n",
    "        df_s[\"antibiotic_on\"] = (\n",
    "            (df_s[\"Time\"] >= t0_ab) & (df_s[\"Time\"] <= t0_ab + dur_ab)\n",
    "        ).astype(int)\n",
    "        out.append(df_s)\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "@register_scenario(\"environmental_drift\")\n",
    "def run_environmental_drift(config, df_init, timepoints,\n",
    "                             comm_meta=None, **kwargs):\n",
    "    \"\"\"\n",
    "    gLV with slowly drifting parameters (non-stationary environment).\n",
    "    r(t) = r_base + drift_r * W_r(t)\n",
    "    A(t) = A_base + drift_A * W_A(t)  (Wiener process, pre-sampled)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(config.seed)\n",
    "    n = config.n_species\n",
    "    r_base, A_base, _ = construct_stable_regime(\n",
    "        n, structure=getattr(config, \"A_structure\", \"sparse\"),\n",
    "        stability_margin=getattr(config, \"stability_margin\", 0.03), rng=rng,\n",
    "        structure_kwargs=getattr(config, \"A_structure_kwargs\",\n",
    "                                  dict(p=0.15, offdiag_std=0.03,\n",
    "                                       diag_range=(-0.6, -0.2))))\n",
    "    drift_r = float(getattr(config, \"drift_rate_r\", 0.01))\n",
    "    drift_A = float(getattr(config, \"drift_rate_A\", 0.005))\n",
    "    m = _build_immigration_vec(config, r_base)\n",
    "    sp = _get_sp_cols(df_init)\n",
    "\n",
    "    # Pre-sample Wiener increments for r and A\n",
    "    n_t = len(timepoints)\n",
    "    dt_arr = np.diff(timepoints)\n",
    "    W_r = np.zeros((n_t, n))\n",
    "    W_A = np.zeros((n_t, n, n))\n",
    "    for k in range(1, n_t):\n",
    "        sqrt_dt = np.sqrt(dt_arr[k-1])\n",
    "        W_r[k] = W_r[k-1] + drift_r * sqrt_dt * rng.normal(0, 1, n)\n",
    "        W_A[k] = W_A[k-1] + drift_A * sqrt_dt * rng.normal(0, 1, (n, n))\n",
    "\n",
    "    out = []\n",
    "    for comm, sub in df_init.groupby(\"Comm_name\"):\n",
    "        x0 = np.maximum(sub[sp].iloc[0].values.astype(float), 0.)\n",
    "        # Step-by-step integration with drifting params\n",
    "        X = np.zeros((n_t, n))\n",
    "        X[0] = x0\n",
    "        for k in range(1, n_t):\n",
    "            r_t = r_base + W_r[k]\n",
    "            A_t = A_base + W_A[k]\n",
    "            # Ensure diagonal remains negative\n",
    "            A_t[np.diag_indices(n)] = -np.abs(np.diag(A_t)) - 1e-6\n",
    "            dt = timepoints[k] - timepoints[k-1]\n",
    "            x = np.maximum(X[k-1], 0.)\n",
    "            dxdt = x * (r_t + A_t @ x) + m\n",
    "            dxdt = _apply_carrying_cap(dxdt, x, config)\n",
    "            dxdt = _apply_allee(dxdt, x, config)\n",
    "            X[k] = np.maximum(x + dt * dxdt, 0.)\n",
    "\n",
    "        df_c = pd.DataFrame(X, columns=sp)\n",
    "        df_c[\"Time\"] = timepoints\n",
    "        df_c[\"Comm_name\"] = comm\n",
    "        out.append(df_c)\n",
    "\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"bistable, dormancy, antibiotic_pulse, environmental_drift registered.\")\n",
    "list_scenarios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Observation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a711  OBSERVATION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def apply_observation_model(df_latent: pd.DataFrame, config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert latent abundances \u2192 sequencing observations.\n",
    "    Modes: continuous | multinomial | dirichlet_multinomial\n",
    "    \"\"\"\n",
    "    mode = getattr(config, \"observation_mode\", \"continuous\")\n",
    "    n_reps = max(int(getattr(config, \"n_replicates\", 1)), 1)\n",
    "\n",
    "    if mode == \"continuous\":\n",
    "        if n_reps == 1:\n",
    "            return df_latent.copy()\n",
    "        out = []\n",
    "        for rep in range(n_reps):\n",
    "            tmp = df_latent.copy()\n",
    "            tmp[\"ReplicateID\"] = rep\n",
    "            out.append(tmp)\n",
    "        return pd.concat(out, ignore_index=True)\n",
    "\n",
    "    rng = np.random.default_rng(getattr(config, \"seed\", 42))\n",
    "    sp = _get_sp_cols(df_latent)\n",
    "    records = []\n",
    "\n",
    "    for _, row in df_latent.iterrows():\n",
    "        abund = row[sp].values.astype(float)\n",
    "        total = abund.sum()\n",
    "        if total <= 0:\n",
    "            continue\n",
    "        p = abund / total\n",
    "\n",
    "        for rep in range(n_reps):\n",
    "            depth = int(np.exp(rng.normal(\n",
    "                np.log(max(getattr(config, \"library_size_mean\", 10000), 1)),\n",
    "                max(getattr(config, \"library_size_sigma\", 0.6), 1e-6)\n",
    "            )))\n",
    "\n",
    "            if mode == \"multinomial\":\n",
    "                counts = rng.multinomial(depth, p)\n",
    "            elif mode == \"dirichlet_multinomial\":\n",
    "                alpha = np.maximum(\n",
    "                    p * float(getattr(config, \"dm_alpha_scale\", 100.)), 1e-12\n",
    "                )\n",
    "                counts = rng.multinomial(depth, rng.dirichlet(alpha))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown observation_mode: {mode}\")\n",
    "\n",
    "            if getattr(config, \"enable_dropout\", False):\n",
    "                counts[counts < int(\n",
    "                    getattr(config, \"detection_limit\", 2)\n",
    "                )] = 0\n",
    "\n",
    "            nr = row.copy()\n",
    "            for i, c in enumerate(sp):\n",
    "                nr[c] = float(counts[i])\n",
    "            nr[\"LibrarySize\"] = depth\n",
    "            nr[\"ReplicateID\"] = rep\n",
    "            nr[\"ObservationType\"] = \"counts\"\n",
    "            records.append(nr)\n",
    "\n",
    "    df_counts = pd.DataFrame(records)\n",
    "    df_rel = df_counts.copy()\n",
    "    tot = df_rel[sp].sum(axis=1).replace(0, np.nan)\n",
    "    for c in sp:\n",
    "        df_rel[c] = df_rel[c] / tot\n",
    "    df_rel[\"ObservationType\"] = \"relative_abundance\"\n",
    "    return pd.concat([df_counts, df_rel], ignore_index=True)\n",
    "\n",
    "\n",
    "def apply_sampling_design(df_latent: pd.DataFrame, config) -> pd.DataFrame:\n",
    "    \"\"\"Apply irregular sampling and/or random missingness.\"\"\"\n",
    "    irregular = getattr(config, \"irregular_sampling\", False)\n",
    "    miss_rate = float(getattr(config, \"missing_rate\", 0.))\n",
    "    if not irregular and miss_rate <= 0.:\n",
    "        return df_latent.copy()\n",
    "    rng = np.random.default_rng(getattr(config, \"seed\", 42))\n",
    "    df = df_latent.copy()\n",
    "\n",
    "    if irregular:\n",
    "        kf = min(max(float(\n",
    "            getattr(config, \"irregular_keep_frac\", 0.6)\n",
    "        ), 0.05), 1.)\n",
    "        kept = []\n",
    "        for _, g in df.groupby(\"Comm_name\", sort=False):\n",
    "            g = g.sort_values(\"Time\")\n",
    "            m = len(g)\n",
    "            if m <= 2:\n",
    "                kept.append(g)\n",
    "                continue\n",
    "            k = max(2, int(round(kf * m)))\n",
    "            mid = np.arange(1, m - 1)\n",
    "            ch = (rng.choice(mid, min(k - 2, len(mid)), replace=False)\n",
    "                  if len(mid) and k > 2\n",
    "                  else np.array([], int))\n",
    "            kept.append(g.iloc[np.sort(np.concatenate([[0, m - 1], ch]))])\n",
    "        df = pd.concat(kept, ignore_index=True)\n",
    "\n",
    "    if miss_rate > 0.:\n",
    "        df = df.loc[\n",
    "            rng.random(len(df)) >= min(max(miss_rate, 0.), 0.95)\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Observation model and sampling design defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Auxiliary Metabolite Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a712  AUXILIARY METABOLITE TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def compute_metabolite_trajectories(df_latent, C, sigma_obs=0.05,\n",
    "                                     rng=None, n_metabolites=None):\n",
    "    \"\"\"\n",
    "    Integrate metabolite ODE: y_m(t) = integral of (C @ x) dt, with noise.\n",
    "    C shape: (n_metabolites, n_species).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    sp = _get_sp_cols(df_latent)\n",
    "    n_s = len(sp)\n",
    "    C = np.asarray(C, float)\n",
    "    if C.ndim == 1:\n",
    "        C = C.reshape(1, n_s)\n",
    "    n_m = C.shape[0]\n",
    "    df_out = df_latent.copy()\n",
    "\n",
    "    for comm, g in df_out.groupby(\"Comm_name\"):\n",
    "        g = g.sort_values(\"Time\")\n",
    "        X = g[sp].values\n",
    "        t = g[\"Time\"].values\n",
    "        Yr = (C @ X.T).T  # (T, M)\n",
    "        Y = np.zeros_like(Yr)\n",
    "        for k in range(1, len(t)):\n",
    "            dt = t[k] - t[k-1]\n",
    "            Y[k] = Y[k-1] + 0.5 * dt * (Yr[k-1] + Yr[k])\n",
    "        Y += rng.normal(0, sigma_obs, Y.shape)\n",
    "        for m_idx in range(n_m):\n",
    "            df_out.loc[g.index, f\"met{m_idx+1}\"] = Y[:, m_idx]\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def compute_shannon_diversity(df, species_prefix=\"sp\"):\n",
    "    \"\"\"Compute Shannon diversity index per row.\"\"\"\n",
    "    sp = [c for c in df.columns if c.startswith(species_prefix)]\n",
    "    X = df[sp].values.astype(float)\n",
    "    X = np.maximum(X, 0.)\n",
    "    totals = X.sum(axis=1, keepdims=True)\n",
    "    totals = np.where(totals == 0, 1, totals)\n",
    "    p = X / totals\n",
    "    p = np.where(p > 0, p, 1)  # avoid log(0)\n",
    "    H = -np.sum(p * np.log(p) * (X > 0), axis=1)\n",
    "    return H\n",
    "\n",
    "\n",
    "print(\"Metabolite trajectories and Shannon diversity defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Benchmark Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a713  BENCHMARK SPLITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def generate_benchmark_splits(df, config, train_frac=0.7, val_frac=0.15,\n",
    "                               seed=None):\n",
    "    \"\"\"Community-level train/val/test splits + regime_ood split.\"\"\"\n",
    "    rng_s = np.random.default_rng(\n",
    "        seed or getattr(config, \"seed\", 42) + 999\n",
    "    )\n",
    "    comms = sorted(df[\"Comm_name\"].unique())\n",
    "    idx = rng_s.permutation(len(comms))\n",
    "    n_tr = int(np.floor(train_frac * len(comms)))\n",
    "    n_va = int(np.floor(val_frac * len(comms)))\n",
    "    tr = [comms[i] for i in idx[:n_tr]]\n",
    "    va = [comms[i] for i in idx[n_tr:n_tr + n_va]]\n",
    "    te = [comms[i] for i in idx[n_tr + n_va:]]\n",
    "    splits = {\n",
    "        \"train\": df[df[\"Comm_name\"].isin(tr)].copy(),\n",
    "        \"val\":   df[df[\"Comm_name\"].isin(va)].copy(),\n",
    "        \"test\":  df[df[\"Comm_name\"].isin(te)].copy(),\n",
    "    }\n",
    "    if \"regime\" in df.columns:\n",
    "        splits[\"regime_ood\"] = df[df[\"regime\"] == 1].copy()\n",
    "    manifest = {\n",
    "        \"train_communities\": tr,\n",
    "        \"val_communities\": va,\n",
    "        \"test_communities\": te,\n",
    "        \"n_train\": len(tr),\n",
    "        \"n_val\": len(va),\n",
    "        \"n_test\": len(te),\n",
    "    }\n",
    "    return splits, manifest\n",
    "\n",
    "\n",
    "print(\"Benchmark splits defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Single gLV Fit (NLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a714  SINGLE gLV FIT (FIX #11: proper NLS instead of finite diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def fit_single_glv(df, n_species, method=\"nls\", lam=1e-3):\n",
    "    \"\"\"\n",
    "    Fit single gLV to (potentially switching) data.\n",
    "\n",
    "    method='nls': Nonlinear least squares \u2014 fits r,A by minimising\n",
    "        || x_data(t) - x_sim(t; r, A) ||^2\n",
    "        Eliminates finite-difference noise artefact.\n",
    "\n",
    "    method='ridge': Original ridge regression on dx/dt (kept for speed).\n",
    "    \"\"\"\n",
    "    sp = [f\"sp{i+1}\" for i in range(n_species)]\n",
    "\n",
    "    if method == \"ridge\":\n",
    "        return _fit_single_glv_ridge(df, n_species, lam)\n",
    "\n",
    "    # \u2500\u2500 NLS method \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # Collect all trajectories\n",
    "    trajs = []\n",
    "    for _, g in df.groupby(\"Comm_name\"):\n",
    "        g = g.sort_values(\"Time\")\n",
    "        X = g[sp].values.astype(float)\n",
    "        t = g[\"Time\"].values.astype(float)\n",
    "        if len(t) < 3:\n",
    "            continue\n",
    "        trajs.append((t, X))\n",
    "\n",
    "    if not trajs:\n",
    "        return np.zeros(n_species), np.zeros((n_species, n_species))\n",
    "\n",
    "    # Pack parameters: r (n) + A (n*n) = n + n^2\n",
    "    n = n_species\n",
    "\n",
    "    def residual(params):\n",
    "        r_hat = params[:n]\n",
    "        A_hat = params[n:].reshape(n, n)\n",
    "        resids = []\n",
    "        for t_arr, X_data in trajs:\n",
    "            x0 = np.maximum(X_data[0], 1e-8)\n",
    "            try:\n",
    "                X_pred = _glv_integrate(r_hat, A_hat, x0, t_arr)\n",
    "                resids.append((X_pred - X_data).ravel())\n",
    "            except RuntimeError:\n",
    "                # If ODE blows up, penalise heavily\n",
    "                resids.append(np.full(X_data.size, 1e3))\n",
    "        return np.concatenate(resids)\n",
    "\n",
    "    # Initial guess via ridge\n",
    "    r0, A0 = _fit_single_glv_ridge(df, n_species, lam)\n",
    "    p0 = np.concatenate([r0, A0.ravel()])\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        result = least_squares(residual, p0, method=\"lm\",\n",
    "                               max_nfev=500, ftol=1e-6, xtol=1e-6)\n",
    "\n",
    "    r_hat = result.x[:n]\n",
    "    A_hat = result.x[n:].reshape(n, n)\n",
    "    return r_hat, A_hat\n",
    "\n",
    "\n",
    "def _fit_single_glv_ridge(df, n_species, lam=1e-3):\n",
    "    \"\"\"Ridge regression baseline (finite differences).\"\"\"\n",
    "    sp = [f\"sp{i+1}\" for i in range(n_species)]\n",
    "    r_hat = np.zeros(n_species)\n",
    "    A_hat = np.zeros((n_species, n_species))\n",
    "\n",
    "    for s in range(n_species):\n",
    "        rows_X, rows_y = [], []\n",
    "        for _, g in df.groupby(\"Comm_name\"):\n",
    "            g = g.sort_values(\"Time\")\n",
    "            X_ = g[sp].values.astype(float)\n",
    "            t_ = g[\"Time\"].values.astype(float)\n",
    "            for k in range(1, len(t_) - 1):\n",
    "                dt = t_[k+1] - t_[k-1]\n",
    "                if dt < 1e-10 or X_[k, s] < 1e-9:\n",
    "                    continue\n",
    "                rows_y.append((X_[k+1, s] - X_[k-1, s]) / dt / X_[k, s])\n",
    "                rows_X.append(np.concatenate([[1.], X_[k]]))\n",
    "        if not rows_X:\n",
    "            continue\n",
    "        Xs = np.array(rows_X)\n",
    "        ys = np.array(rows_y)\n",
    "        th = np.linalg.solve(\n",
    "            Xs.T @ Xs + lam * np.eye(Xs.shape[1]), Xs.T @ ys\n",
    "        )\n",
    "        r_hat[s] = th[0]\n",
    "        A_hat[s, :] = th[1:]\n",
    "\n",
    "    return r_hat, A_hat\n",
    "\n",
    "\n",
    "print(\"Single-gLV fit (NLS + ridge) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a715  VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def _colors(n):\n",
    "    cyc = plt.rcParams[\"axes.prop_cycle\"].by_key().get(\"color\", [\"C0\"] * n)\n",
    "    return (cyc * ((n // len(cyc)) + 1))[:n]\n",
    "\n",
    "\n",
    "def plot_regime_switching(df, n_communities=3, trigger_col=None,\n",
    "                           trigger_threshold=None, regime_col=\"regime\",\n",
    "                           species_prefix=\"sp\", figsize_per_comm=(12, 4),\n",
    "                           title=\"\"):\n",
    "    \"\"\"\n",
    "    Plot community trajectories with regime background shading.\n",
    "    Optional bottom panel shows hidden trigger variable.\n",
    "    \"\"\"\n",
    "    sp_cols = [c for c in df.columns if c.startswith(species_prefix)]\n",
    "    n_sp = len(sp_cols)\n",
    "    has_trig = trigger_col and trigger_col in df.columns\n",
    "    has_reg = regime_col in df.columns\n",
    "    colors = _colors(n_sp)\n",
    "    comms = sorted(df[\"Comm_name\"].unique())[:n_communities]\n",
    "    n_panels = 2 if has_trig else 1\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        len(comms) * n_panels, 1,\n",
    "        figsize=(figsize_per_comm[0],\n",
    "                 figsize_per_comm[1] * len(comms)),\n",
    "        squeeze=False\n",
    "    )\n",
    "\n",
    "    for ci, comm in enumerate(comms):\n",
    "        g = df[df[\"Comm_name\"] == comm].sort_values(\"Time\")\n",
    "        t = g[\"Time\"].values\n",
    "        ax = axes[ci * n_panels, 0]\n",
    "\n",
    "        for si, col in enumerate(sp_cols):\n",
    "            ax.plot(t, g[col].values, color=colors[si], lw=1.5, label=col)\n",
    "\n",
    "        if has_reg:\n",
    "            reg = g[regime_col].values\n",
    "            ts = t[0]\n",
    "            cr = reg[0]\n",
    "            for k in range(1, len(t)):\n",
    "                if reg[k] != cr or k == len(t) - 1:\n",
    "                    color = \"#FFD580\" if cr == 1 else \"#E8F4E8\"\n",
    "                    ax.axvspan(ts, t[k], alpha=0.22, color=color, zorder=0)\n",
    "                    ts = t[k]\n",
    "                    cr = reg[k]\n",
    "        if \"t_switch_true\" in g.columns:\n",
    "            ax.axvline(g[\"t_switch_true\"].iloc[0], color=\"k\",\n",
    "                       lw=1, ls=\"--\", alpha=0.6)\n",
    "\n",
    "        ax.set_ylabel(\"Abundance\")\n",
    "        ax.grid(True, lw=0.4, alpha=0.5)\n",
    "        ax.set_title(\n",
    "            f\"{comm}\" + (f\" \u2014 {title}\" if title and ci == 0 else \"\"),\n",
    "            loc=\"left\"\n",
    "        )\n",
    "        if ci == 0:\n",
    "            ax.legend(fontsize=7, ncol=min(3, n_sp), loc=\"upper right\")\n",
    "\n",
    "        if has_trig:\n",
    "            ax2 = axes[ci * n_panels + 1, 0]\n",
    "            ax2.plot(t, g[trigger_col].values, color=\"steelblue\", lw=1.5)\n",
    "            if trigger_threshold is not None:\n",
    "                ax2.axhline(trigger_threshold, color=\"crimson\", ls=\"--\",\n",
    "                             lw=1., label=f\"threshold={trigger_threshold}\")\n",
    "                ax2.legend(fontsize=7)\n",
    "            ax2.set_ylabel(trigger_col)\n",
    "            ax2.set_xlabel(\"Time\")\n",
    "            ax2.grid(True, lw=0.4, alpha=0.5)\n",
    "        else:\n",
    "            ax.set_xlabel(\"Time\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_single_vs_switching(df_true, n_species, timepoints,\n",
    "                              community=None, method=\"nls\", lam=1e-3):\n",
    "    \"\"\"\n",
    "    Fit single gLV to switching data and compare trajectories + RMSE.\n",
    "    \"\"\"\n",
    "    sp = [f\"sp{i+1}\" for i in range(n_species)]\n",
    "    r_hat, A_hat = fit_single_glv(df_true, n_species, method=method, lam=lam)\n",
    "    if community is None:\n",
    "        community = sorted(df_true[\"Comm_name\"].unique())[0]\n",
    "    g = df_true[df_true[\"Comm_name\"] == community].sort_values(\"Time\")\n",
    "    x0 = np.maximum(g[sp].iloc[0].values.astype(float), 1e-8)\n",
    "    X_pred = _glv_integrate(r_hat, A_hat, x0, timepoints)\n",
    "    colors = _colors(n_species)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "    # Left: true switching\n",
    "    ax = axes[0]\n",
    "    for si, col in enumerate(sp):\n",
    "        ax.plot(g[\"Time\"], g[col], color=colors[si], lw=1.5, label=col)\n",
    "    if \"regime\" in g.columns:\n",
    "        reg = g[\"regime\"].values\n",
    "        t_ = g[\"Time\"].values\n",
    "        ts = t_[0]\n",
    "        cr = reg[0]\n",
    "        for k in range(1, len(t_)):\n",
    "            if reg[k] != cr or k == len(t_) - 1:\n",
    "                ax.axvspan(ts, t_[k], alpha=0.2,\n",
    "                           color=\"#FFD580\" if cr == 1 else \"#E8F4E8\",\n",
    "                           zorder=0)\n",
    "                ts = t_[k]\n",
    "                cr = reg[k]\n",
    "    ax.set_title(f\"True 2-Regime ({community})\", loc=\"left\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Abundance\")\n",
    "    ax.legend(fontsize=7, ncol=2)\n",
    "    ax.grid(True, lw=0.4, alpha=0.5)\n",
    "\n",
    "    # Right: single gLV prediction\n",
    "    ax = axes[1]\n",
    "    for si, col in enumerate(sp):\n",
    "        ax.plot(timepoints, X_pred[:, si], color=colors[si],\n",
    "                lw=1.5, ls=\"--\", label=col)\n",
    "        ax.scatter(g[\"Time\"], g[col], color=colors[si],\n",
    "                   s=20, alpha=0.6, zorder=4)\n",
    "    g_interp = np.array([\n",
    "        np.interp(timepoints, g[\"Time\"].values, g[col].values)\n",
    "        for col in sp\n",
    "    ]).T\n",
    "    rmse = np.sqrt(np.mean((X_pred - g_interp)**2, axis=0))\n",
    "    ax.set_title(f\"Single gLV fit \u2014 mean RMSE={rmse.mean():.4f}\", loc=\"left\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.grid(True, lw=0.4, alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    return fig, rmse\n",
    "\n",
    "\n",
    "print(\"Visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Export System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a716  EXPORT (FIX #12: ground-truth params; FIX #13: per-comm switch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def export_config_snapshot(config_obj, output_dir=\".\", verbose=True):\n",
    "    \"\"\"Save GeneratorConfig as JSON.\"\"\"\n",
    "    p = Path(output_dir)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    snap = asdict(config_obj)\n",
    "    snap[\"timestamp\"] = datetime.now().isoformat()\n",
    "    fp = p / \"config.json\"\n",
    "    with open(fp, \"w\") as f:\n",
    "        json.dump(snap, f, indent=2, default=str)\n",
    "    if verbose:\n",
    "        print(f\"Config saved \u2192 {fp}\")\n",
    "    return str(fp)\n",
    "\n",
    "\n",
    "def export_dataset(config, df_init, timepoints, output_dir=\"test_data\",\n",
    "                   comm_meta=None, compute_metabolites=False,\n",
    "                   metabolite_C=None):\n",
    "    \"\"\"\n",
    "    Run scenario and export full dataset bundle:\n",
    "      latent_truth.csv, observed_counts.csv, config.json,\n",
    "      dataset_metadata.json, split_manifest.json,\n",
    "      ground_truth_params.json (FIX #12),\n",
    "      split_train/val/test/regime_ood.csv,\n",
    "      [optional] metabolites_truth.csv\n",
    "    \"\"\"\n",
    "    outdir = Path(output_dir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    sc = config.scenario\n",
    "    if sc not in SCENARIO_REGISTRY:\n",
    "        raise ValueError(f\"Scenario '{sc}' not registered.\")\n",
    "\n",
    "    _GROUND_TRUTH_PARAMS.clear()\n",
    "\n",
    "    df_latent_raw = SCENARIO_REGISTRY[sc](\n",
    "        config, df_init=df_init, timepoints=timepoints,\n",
    "        comm_meta=comm_meta\n",
    "    )\n",
    "    df_latent = apply_sampling_design(df_latent_raw, config)\n",
    "    df_observed = apply_observation_model(df_latent, config)\n",
    "\n",
    "    paths = {}\n",
    "\n",
    "    def _save(df, name):\n",
    "        p = outdir / name\n",
    "        df.to_csv(p, index=False)\n",
    "        paths[name.replace(\".csv\", \"\")] = str(p)\n",
    "\n",
    "    _save(df_latent, \"latent_truth.csv\")\n",
    "    _save(df_observed, \"observed_counts.csv\")\n",
    "\n",
    "    # Config\n",
    "    snap = asdict(config)\n",
    "    snap[\"timestamp\"] = datetime.now().isoformat()\n",
    "    with open(outdir / \"config.json\", \"w\") as f:\n",
    "        json.dump(snap, f, indent=2, default=str)\n",
    "    paths[\"config\"] = str(outdir / \"config.json\")\n",
    "\n",
    "    # FIX #12: ground-truth regime parameters\n",
    "    if getattr(config, \"export_ground_truth_params\", True) and _GROUND_TRUTH_PARAMS:\n",
    "        with open(outdir / \"ground_truth_params.json\", \"w\") as f:\n",
    "            json.dump(_GROUND_TRUTH_PARAMS, f, indent=2)\n",
    "        paths[\"ground_truth_params\"] = str(outdir / \"ground_truth_params.json\")\n",
    "\n",
    "    # Metabolites\n",
    "    if compute_metabolites:\n",
    "        n_sp = config.n_species\n",
    "        C = (metabolite_C if metabolite_C is not None\n",
    "             else np.abs(np.random.default_rng(config.seed).normal(\n",
    "                 0, .3, (1, n_sp))))\n",
    "        df_met = compute_metabolite_trajectories(df_latent, C=C)\n",
    "        _save(df_met, \"metabolites_truth.csv\")\n",
    "\n",
    "    # Splits\n",
    "    splits, manifest = generate_benchmark_splits(df_observed, config)\n",
    "    for sname, df_s in splits.items():\n",
    "        _save(df_s, f\"split_{sname}.csv\")\n",
    "    with open(outdir / \"split_manifest.json\", \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    paths[\"split_manifest\"] = str(outdir / \"split_manifest.json\")\n",
    "\n",
    "    # FIX #13: per-community switch-time estimation\n",
    "    latent_vars = [c for c in [\"u\", \"A_M\", \"w\", \"regime\", \"t_switch_true\"]\n",
    "                   if c in df_latent.columns]\n",
    "    meta = {\n",
    "        \"scenario\": sc,\n",
    "        \"seed\": config.seed,\n",
    "        \"regime_distance\": getattr(config, \"regime_distance\", None),\n",
    "        \"n_rows_observed\": len(df_observed),\n",
    "        \"n_communities\": df_observed[\"Comm_name\"].nunique(),\n",
    "        \"time_min\": float(df_observed[\"Time\"].min()),\n",
    "        \"time_max\": float(df_observed[\"Time\"].max()),\n",
    "        \"n_timepoints\": int(df_observed[\"Time\"].nunique()),\n",
    "        \"has_replicates\": \"ReplicateID\" in df_observed.columns,\n",
    "        \"latent_variables\": latent_vars,\n",
    "    }\n",
    "\n",
    "    # Per-community switch times (FIX #13)\n",
    "    if \"t_switch_true\" in df_latent.columns:\n",
    "        per_comm_switches = {}\n",
    "        for comm, g in df_latent.groupby(\"Comm_name\"):\n",
    "            t_sw = g[\"t_switch_true\"].dropna().unique()\n",
    "            if len(t_sw) > 0:\n",
    "                per_comm_switches[comm] = float(t_sw[0])\n",
    "        meta[\"per_community_switch_times\"] = per_comm_switches\n",
    "    elif \"w\" in df_latent.columns:\n",
    "        per_comm_switches = {}\n",
    "        for comm, g in df_latent.groupby(\"Comm_name\"):\n",
    "            g = g.sort_values(\"Time\")\n",
    "            idx_w = (g[\"w\"] - 0.5).abs().idxmin()\n",
    "            per_comm_switches[comm] = float(g.loc[idx_w, \"Time\"])\n",
    "        meta[\"per_community_switch_times\"] = per_comm_switches\n",
    "\n",
    "    with open(outdir / \"dataset_metadata.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    paths[\"metadata\"] = str(outdir / \"dataset_metadata.json\")\n",
    "\n",
    "    print(f\"\u2705 Export complete \u2192 {outdir.resolve()}\")\n",
    "    for k, v in paths.items():\n",
    "        print(f\"  {k:30s}: {v}\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "print(\"Export system defined (with ground-truth params & per-comm switches).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Dataset Sweep Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a717  DATASET SWEEP FACTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "DIFFICULTY_PRESETS = dict(\n",
    "    easy   = dict(regime_distance=1.5, seed_offset=0),\n",
    "    medium = dict(regime_distance=1.0, seed_offset=1000),\n",
    "    hard   = dict(regime_distance=0.3, seed_offset=2000),\n",
    ")\n",
    "\n",
    "SWITCHING_SCENARIOS = [\n",
    "    \"hidden_trigger\", \"cumulative_trigger\", \"time_switch\", \"soft_switch\"\n",
    "]\n",
    "\n",
    "\n",
    "def run_dataset_sweep(df_init, timepoints, base_config, root_dir=\"datasets\",\n",
    "                      comm_meta=None, scenarios=None, difficulties=None,\n",
    "                      verbose=True):\n",
    "    \"\"\"Generate all scenario \u00d7 difficulty datasets.\"\"\"\n",
    "    ROOT = Path(root_dir)\n",
    "    ROOT.mkdir(exist_ok=True)\n",
    "    scenarios = scenarios or SWITCHING_SCENARIOS\n",
    "    difficulties = difficulties or DIFFICULTY_PRESETS\n",
    "\n",
    "    for scenario in scenarios:\n",
    "        for difficulty, params in difficulties.items():\n",
    "            cfg = deepcopy(base_config)\n",
    "            cfg.scenario = scenario\n",
    "            cfg.regime_distance = params[\"regime_distance\"]\n",
    "            cfg.seed = base_config.seed + params[\"seed_offset\"]\n",
    "            cfg.observation_mode = \"continuous\"\n",
    "            cfg.n_replicates = 1\n",
    "            for k, v in dict(\n",
    "                t_switch=10., theta=.5, k_u=.4, u0=0., epsilon=.1,\n",
    "                idx_M=None, c1=1., M_init=.05, a0=0.\n",
    "            ).items():\n",
    "                setattr(cfg, k, v)\n",
    "            if verbose:\n",
    "                print(f\"\\n=== {scenario}/{difficulty} \"\n",
    "                      f\"(rd={params['regime_distance']}) ===\")\n",
    "            try:\n",
    "                export_dataset(cfg, df_init, timepoints,\n",
    "                               str(ROOT / scenario / difficulty), comm_meta)\n",
    "            except Exception as e:\n",
    "                print(f\"  \u26a0\ufe0f Failed: {e}\")\n",
    "\n",
    "    print(f\"\\n\u2705 Sweep complete \u2192 {ROOT.resolve()}\")\n",
    "\n",
    "\n",
    "print(\"Dataset sweep factory defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Validation Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a718  VALIDATION SUITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def validate_dataset(df, config, verbose=True):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of a generated dataset.\n",
    "    Returns dict of {check_name: passed_bool}.\n",
    "    \"\"\"\n",
    "    sp = _get_sp_cols(df)\n",
    "    results = {}\n",
    "\n",
    "    # 1. No NaN / Inf in species columns\n",
    "    has_nan = df[sp].isna().any().any()\n",
    "    has_inf = np.isinf(df[sp].values.astype(float)).any()\n",
    "    results[\"no_nan\"] = not has_nan\n",
    "    results[\"no_inf\"] = not has_inf\n",
    "\n",
    "    # 2. Non-negative abundances\n",
    "    results[\"nonnegative\"] = (df[sp].values.astype(float) >= -1e-12).all()\n",
    "\n",
    "    # 3. Community count\n",
    "    results[\"communities_present\"] = df[\"Comm_name\"].nunique() > 0\n",
    "\n",
    "    # 4. Time monotonicity per community\n",
    "    time_ok = True\n",
    "    for _, g in df.groupby(\"Comm_name\"):\n",
    "        t = g[\"Time\"].values.astype(float)\n",
    "        if not np.all(np.diff(t) >= -1e-10):\n",
    "            time_ok = False\n",
    "            break\n",
    "    results[\"time_monotonic\"] = time_ok\n",
    "\n",
    "    # 5. No blow-up (species < 100 as sanity bound)\n",
    "    max_val = df[sp].values.astype(float).max()\n",
    "    results[\"no_blowup\"] = max_val < 100.\n",
    "\n",
    "    # 6. Regime labels valid (if present)\n",
    "    if \"regime\" in df.columns:\n",
    "        results[\"valid_regimes\"] = set(df[\"regime\"].unique()).issubset({0, 1})\n",
    "\n",
    "    # 7. Observation model checks (if counts present)\n",
    "    if \"ObservationType\" in df.columns:\n",
    "        counts_df = df[df[\"ObservationType\"] == \"counts\"]\n",
    "        if len(counts_df) > 0:\n",
    "            counts_arr = counts_df[sp].values.astype(float)\n",
    "            results[\"counts_nonneg\"] = (counts_arr >= 0).all()\n",
    "            results[\"counts_integer\"] = np.allclose(\n",
    "                counts_arr, np.round(counts_arr)\n",
    "            )\n",
    "\n",
    "    if verbose:\n",
    "        for check, passed in results.items():\n",
    "            status = \"\u2705\" if passed else \"\u274c\"\n",
    "            print(f\"  {status} {check}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Validation suite defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a719  TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# \u2500\u2500 Setup shared test objects \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "n_species   = 5\n",
    "size_counts = [(1, 3), (2, 4), (3, 2)]\n",
    "timepoints  = np.linspace(0., 20., 101)\n",
    "\n",
    "df_init, COMM_META = generate_community_dataframe(\n",
    "    n_species=n_species, size_counts=size_counts,\n",
    "    mean_abundance=0.01, bounds=(0.001, 0.1), sigma_log=1.0, seed=42\n",
    ")\n",
    "\n",
    "sp_cols = [f\"sp{i+1}\" for i in range(n_species)]\n",
    "\n",
    "CONFIG.n_species           = n_species\n",
    "CONFIG.A_structure         = \"sparse\"\n",
    "CONFIG.A_structure_kwargs  = dict(p=0.15, offdiag_std=0.03,\n",
    "                                   diag_range=(-0.6, -0.2))\n",
    "CONFIG.stability_margin    = 0.03\n",
    "CONFIG.enable_hierarchical = True\n",
    "CONFIG.sigma_r = 0.15\n",
    "CONFIG.sigma_A = 0.05\n",
    "\n",
    "# Test 0: stable regime construction\n",
    "r_t, A_t, x_t = construct_stable_regime(\n",
    "    n_species, rng=np.random.default_rng(2026)\n",
    ")\n",
    "assert np.max(np.abs(r_t + A_t @ x_t)) < 1e-10\n",
    "J_t = np.diag(x_t) @ A_t\n",
    "assert float(np.max(np.real(np.linalg.eigvals(J_t)))) < -0.03\n",
    "print(\"\u2705 Test 0: construct_stable_regime passed.\")\n",
    "\n",
    "# Test 1: community dataframe\n",
    "for nm, row in df_init.iterrows():\n",
    "    cn = df_init.loc[nm, \"Comm_name\"]\n",
    "    absent = [i for i in range(n_species) if i not in COMM_META[cn]]\n",
    "    for a in absent:\n",
    "        assert df_init.loc[nm, f\"sp{a+1}\"] == 0.\n",
    "covered = {sp for pr in COMM_META.values() for sp in pr}\n",
    "assert covered == set(range(n_species))\n",
    "print(f\"\u2705 Test 1: community df: {len(df_init)} rows, all species covered.\")\n",
    "\n",
    "# Test 2: baseline_glv_stable\n",
    "CONFIG.scenario = \"baseline_glv_stable\"\n",
    "CONFIG.observation_mode = \"continuous\"\n",
    "CONFIG.n_replicates = 1\n",
    "\n",
    "df_stable = simulate_community(\n",
    "    CONFIG, df_init=df_init, timepoints=timepoints, comm_meta=COMM_META\n",
    ")\n",
    "assert (df_stable[_get_sp_cols(df_stable)] >= -1e-12).all().all()\n",
    "print(f\"\u2705 Test 2: baseline_glv_stable: {df_stable.shape}\")\n",
    "\n",
    "# Test 3: All switching scenarios\n",
    "for scenario, params in [\n",
    "    (\"time_switch\",        {\"t_switch\": 10., \"regime_distance\": 1.}),\n",
    "    (\"soft_switch\",        {\"t_switch\": 10., \"epsilon\": 1.5,\n",
    "                            \"regime_distance\": 1.}),\n",
    "    (\"hidden_trigger\",     {\"theta\": .5, \"k_u\": .4, \"u0\": 0.,\n",
    "                            \"regime_distance\": 1.}),\n",
    "    (\"cumulative_trigger\", {\"idx_M\": [0, 2], \"c1\": 1., \"M_init\": .05,\n",
    "                            \"a0\": 0., \"regime_distance\": 1.}),\n",
    "]:\n",
    "    CONFIG.scenario = scenario\n",
    "    for k, v in params.items():\n",
    "        setattr(CONFIG, k, v)\n",
    "    CONFIG.observation_mode = \"continuous\"\n",
    "    df_s = simulate_community(\n",
    "        CONFIG, df_init=df_init, timepoints=timepoints, comm_meta=COMM_META\n",
    "    )\n",
    "    sp_c = _get_sp_cols(df_s)\n",
    "    assert (df_s[sp_c] >= -1e-12).all().all()\n",
    "    assert \"regime\" in df_s.columns or \"w\" in df_s.columns\n",
    "    print(f\"  \u2705 {scenario}: {df_s.shape}\")\n",
    "print(\"\u2705 Test 3: All switching scenarios passed.\")\n",
    "\n",
    "# Test 4: observation model\n",
    "CONFIG.scenario = \"baseline_glv_stable\"\n",
    "CONFIG.observation_mode = \"dirichlet_multinomial\"\n",
    "CONFIG.library_size_mean = 10000\n",
    "CONFIG.library_size_sigma = 0.6\n",
    "CONFIG.dm_alpha_scale = 100.\n",
    "CONFIG.enable_dropout = True\n",
    "CONFIG.detection_limit = 2\n",
    "CONFIG.n_replicates = 2\n",
    "\n",
    "df_obs = simulate_community(\n",
    "    CONFIG, df_init=df_init, timepoints=timepoints, comm_meta=COMM_META\n",
    ")\n",
    "assert set(df_obs[\"ObservationType\"].unique()) == {\"counts\", \"relative_abundance\"}\n",
    "print(f\"\u2705 Test 4: observation model: {df_obs.shape}\")\n",
    "\n",
    "# Test 5: immigration \u2014 absent species can colonise\n",
    "CONFIG.scenario = \"baseline_glv_stable\"\n",
    "CONFIG.observation_mode = \"continuous\"\n",
    "CONFIG.n_replicates = 1\n",
    "CONFIG.immigration_rate = 1e-3\n",
    "CONFIG.immigration_scale = 1.0\n",
    "\n",
    "df_imm_init = pd.DataFrame(\n",
    "    [[\"single_sp1\", 0.0] + [0.05, 0.0, 0.0, 0.0, 0.0]],\n",
    "    columns=[\"Comm_name\", \"Time\"] + [f\"sp{i+1}\" for i in range(5)]\n",
    ")\n",
    "df_imm = simulate_community(\n",
    "    CONFIG, df_init=df_imm_init, timepoints=timepoints,\n",
    "    comm_meta={\"single_sp1\": [0]}\n",
    ")\n",
    "sp_c = _get_sp_cols(df_imm)\n",
    "final = df_imm[np.isclose(df_imm[\"Time\"], timepoints[-1])][sp_c].values[0]\n",
    "n_colonised = int(np.sum(final > 1e-6))\n",
    "assert n_colonised >= 1\n",
    "print(f\"\u2705 Test 5: immigration \u2014 {n_colonised}/{len(sp_c)} species at t_final\")\n",
    "CONFIG.immigration_rate = 1e-4  # reset\n",
    "\n",
    "# Test 6: hidden trigger heterogeneity\n",
    "CONFIG.scenario = \"hidden_trigger\"\n",
    "CONFIG.trigger_sigma_theta = 0.1\n",
    "CONFIG.trigger_sigma_k_u = 0.05\n",
    "CONFIG.regime_distance = 1.0\n",
    "CONFIG.observation_mode = \"continuous\"\n",
    "CONFIG.n_replicates = 1\n",
    "\n",
    "df_het = simulate_community(\n",
    "    CONFIG, df_init=df_init, timepoints=timepoints, comm_meta=COMM_META\n",
    ")\n",
    "if \"t_switch_true\" in df_het.columns:\n",
    "    switch_times = df_het.groupby(\"Comm_name\")[\"t_switch_true\"].first().dropna()\n",
    "    if len(switch_times) > 1:\n",
    "        assert switch_times.std() > 0, \"Trigger heterogeneity should spread switch times\"\n",
    "        print(f\"  Switch time spread: {switch_times.std():.3f}\")\n",
    "print(\"\u2705 Test 6: hidden trigger heterogeneity passed.\")\n",
    "CONFIG.trigger_sigma_theta = 0.0  # reset\n",
    "CONFIG.trigger_sigma_k_u = 0.0\n",
    "\n",
    "# Test 7: validation suite\n",
    "CONFIG.scenario = \"time_switch\"\n",
    "CONFIG.t_switch = 10.\n",
    "CONFIG.regime_distance = 1.\n",
    "CONFIG.observation_mode = \"continuous\"\n",
    "CONFIG.n_replicates = 1\n",
    "df_val = simulate_community(\n",
    "    CONFIG, df_init=df_init, timepoints=timepoints, comm_meta=COMM_META\n",
    ")\n",
    "print(\"Validation results:\")\n",
    "vr = validate_dataset(df_val, CONFIG)\n",
    "assert all(vr.values()), \"Validation failed!\"\n",
    "print(\"\u2705 Test 7: validation suite passed.\")\n",
    "\n",
    "# Test 8: environmental drift\n",
    "CONFIG.scenario = \"environmental_drift\"\n",
    "CONFIG.enable_drift = True\n",
    "CONFIG.drift_rate_r = 0.01\n",
    "CONFIG.drift_rate_A = 0.005\n",
    "df_drift = simulate_community(\n",
    "    CONFIG, df_init=df_init, timepoints=timepoints, comm_meta=COMM_META\n",
    ")\n",
    "assert (df_drift[_get_sp_cols(df_drift)] >= -1e-12).all().all()\n",
    "print(f\"\u2705 Test 8: environmental_drift: {df_drift.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL TESTS PASSED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Demo: Ground Truth vs Observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \u00a720  COMPREHENSIVE DEMO FIGURE: Ground Truth vs Observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING COMPREHENSIVE DEMO FIGURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use more communities with richer composition\n",
    "n_species_demo = 5\n",
    "size_counts_demo = [(2, 5), (3, 5), (4, 3), (5, 2)]\n",
    "timepoints_demo = np.linspace(0., 20., 101)\n",
    "\n",
    "df_init_demo, COMM_META_DEMO = generate_community_dataframe(\n",
    "    n_species=n_species_demo, size_counts=size_counts_demo,\n",
    "    mean_abundance=0.01, bounds=(0.001, 0.1), sigma_log=1.0, seed=99)\n",
    "\n",
    "CONFIG.n_species = n_species_demo\n",
    "CONFIG.scenario = \"hidden_trigger\"\n",
    "CONFIG.regime_distance = 1.0\n",
    "CONFIG.theta = 0.5; CONFIG.k_u = 0.4; CONFIG.u0 = 0.0; CONFIG.epsilon = 0.1\n",
    "CONFIG.trigger_sigma_theta = 0.1; CONFIG.trigger_sigma_k_u = 0.05\n",
    "CONFIG.immigration_rate = 1e-4; CONFIG.immigration_scale = 1.0\n",
    "CONFIG.enable_carrying_cap = False; CONFIG.enable_allee = False\n",
    "CONFIG.observation_mode = \"continuous\"; CONFIG.n_replicates = 1\n",
    "\n",
    "df_latent = SCENARIO_REGISTRY[\"hidden_trigger\"](\n",
    "    CONFIG, df_init=df_init_demo, timepoints=timepoints_demo,\n",
    "    comm_meta=COMM_META_DEMO)\n",
    "\n",
    "# Observed\n",
    "obs_config = deepcopy(CONFIG)\n",
    "obs_config.observation_mode = \"dirichlet_multinomial\"\n",
    "obs_config.library_size_mean = 5000; obs_config.library_size_sigma = 0.5\n",
    "obs_config.dm_alpha_scale = 80.0; obs_config.enable_dropout = True\n",
    "obs_config.detection_limit = 3; obs_config.n_replicates = 3\n",
    "df_observed = apply_observation_model(df_latent, obs_config)\n",
    "\n",
    "# Metabolite + diversity\n",
    "C_met = np.abs(np.random.default_rng(77).normal(0, .3, (1, n_species_demo)))\n",
    "df_latent_met = compute_metabolite_trajectories(df_latent, C=C_met, sigma_obs=0.01)\n",
    "df_latent[\"Shannon\"] = compute_shannon_diversity(df_latent)\n",
    "\n",
    "# Pick 3 communities with >= 3 species\n",
    "multi_sp_comms = [c for c, idxs in COMM_META_DEMO.items() if len(idxs) >= 3]\n",
    "comms_to_plot = sorted(multi_sp_comms)[:3]\n",
    "sp = [f\"sp{i+1}\" for i in range(n_species_demo)]\n",
    "colors = _colors(n_species_demo)\n",
    "n_comms = len(comms_to_plot)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 4.2 * n_comms))\n",
    "outer_gs = gridspec.GridSpec(n_comms, 1, hspace=0.38)\n",
    "\n",
    "for ci, comm in enumerate(comms_to_plot):\n",
    "    inner_gs = gridspec.GridSpecFromSubplotSpec(\n",
    "        4, 1, subplot_spec=outer_gs[ci],\n",
    "        height_ratios=[3, 3, 1.5, 1.5], hspace=0.3)\n",
    "\n",
    "    g_lat = df_latent[df_latent[\"Comm_name\"] == comm].sort_values(\"Time\")\n",
    "    g_met = df_latent_met[df_latent_met[\"Comm_name\"] == comm].sort_values(\"Time\")\n",
    "    t_lat = g_lat[\"Time\"].values.astype(float)\n",
    "\n",
    "    def _shade(ax, g, t_arr, alpha=0.22):\n",
    "        if \"regime\" not in g.columns: return\n",
    "        reg = g[\"regime\"].values; ts = t_arr[0]; cr = reg[0]\n",
    "        for k in range(1, len(t_arr)):\n",
    "            if reg[k] != cr or k == len(t_arr) - 1:\n",
    "                ax.axvspan(ts, t_arr[k], alpha=alpha,\n",
    "                           color=\"#FFD580\" if cr == 1 else \"#E8F4E8\", zorder=0)\n",
    "                ts = t_arr[k]; cr = reg[k]\n",
    "\n",
    "    # Panel A: Latent truth\n",
    "    ax_a = fig.add_subplot(inner_gs[0])\n",
    "    for si, col in enumerate(sp):\n",
    "        ax_a.plot(t_lat, g_lat[col].values, color=colors[si], lw=1.8,\n",
    "                  label=col, zorder=3)\n",
    "    _shade(ax_a, g_lat, t_lat)\n",
    "    if \"t_switch_true\" in g_lat.columns:\n",
    "        t_sw = g_lat[\"t_switch_true\"].iloc[0]\n",
    "        if pd.notna(t_sw):\n",
    "            ax_a.axvline(t_sw, color=\"k\", lw=1.2, ls=\"--\", alpha=0.6,\n",
    "                         label=f\"switch t={t_sw:.1f}\")\n",
    "    ax_a.set_ylabel(\"Abundance\\n(latent truth)\", fontsize=9)\n",
    "    ax_a.set_title(f\"{comm} \u2014 Ground Truth vs Observed  \"\n",
    "                   f\"(species present: {COMM_META_DEMO[comm]})\",\n",
    "                   loc=\"left\", fontweight=\"bold\", fontsize=11)\n",
    "    ax_a.legend(fontsize=7, ncol=min(4, n_species_demo + 1), loc=\"upper right\")\n",
    "    ax_a.grid(True, lw=0.3, alpha=0.4); ax_a.tick_params(labelbottom=False)\n",
    "\n",
    "    # Panel B: Observed\n",
    "    ax_b = fig.add_subplot(inner_gs[1])\n",
    "    df_obs_comm = df_observed[\n",
    "        (df_observed[\"Comm_name\"] == comm) &\n",
    "        (df_observed[\"ObservationType\"] == \"relative_abundance\")]\n",
    "    markers = [\"o\", \"s\", \"^\"]\n",
    "    for rep_id in sorted(df_obs_comm[\"ReplicateID\"].unique()):\n",
    "        df_rep = df_obs_comm[df_obs_comm[\"ReplicateID\"] == rep_id].sort_values(\"Time\")\n",
    "        t_obs = df_rep[\"Time\"].values.astype(float)\n",
    "        for si, col in enumerate(sp):\n",
    "            ax_b.scatter(t_obs, df_rep[col].values.astype(float),\n",
    "                         color=colors[si], marker=markers[int(rep_id) % 3],\n",
    "                         s=14, alpha=0.45, edgecolors=\"none\", zorder=3)\n",
    "    # latent compositional reference\n",
    "    lat_total = np.maximum(g_lat[sp].values.astype(float).sum(axis=1), 1e-15)\n",
    "    for si, col in enumerate(sp):\n",
    "        ax_b.plot(t_lat, g_lat[col].values.astype(float) / lat_total,\n",
    "                  color=colors[si], lw=0.8, ls=\"--\", alpha=0.5)\n",
    "    _shade(ax_b, g_lat, t_lat, alpha=0.15)\n",
    "    ax_b.set_ylabel(\"Rel. Abundance\\n(DM observed)\", fontsize=9)\n",
    "    ax_b.set_ylim(-0.05, 1.05)\n",
    "    ax_b.grid(True, lw=0.3, alpha=0.4); ax_b.tick_params(labelbottom=False)\n",
    "\n",
    "    # Panel C: Hidden u(t)\n",
    "    ax_c = fig.add_subplot(inner_gs[2])\n",
    "    if \"u\" in g_lat.columns:\n",
    "        ax_c.plot(t_lat, g_lat[\"u\"].values, color=\"steelblue\", lw=1.5)\n",
    "        ax_c.axhline(CONFIG.theta, color=\"crimson\", ls=\"--\", lw=1,\n",
    "                      label=f\"\u03b8={CONFIG.theta}\")\n",
    "        ax_c.legend(fontsize=7, loc=\"lower right\")\n",
    "    ax_c.set_ylabel(\"u(t)\", fontsize=9)\n",
    "    ax_c.grid(True, lw=0.3, alpha=0.4); ax_c.tick_params(labelbottom=False)\n",
    "\n",
    "    # Panel D: Metabolite + diversity\n",
    "    ax_d = fig.add_subplot(inner_gs[3])\n",
    "    if \"met1\" in g_met.columns:\n",
    "        ax_d.plot(g_met[\"Time\"].values, g_met[\"met1\"].values,\n",
    "                  color=\"darkorange\", lw=1.5, label=\"Metabolite\")\n",
    "    ax_d2 = ax_d.twinx()\n",
    "    ax_d2.plot(t_lat, g_lat[\"Shannon\"].values, color=\"purple\", lw=1.2,\n",
    "               ls=\":\", label=\"Shannon H\")\n",
    "    ax_d.set_ylabel(\"Metabolite\", fontsize=9, color=\"darkorange\")\n",
    "    ax_d2.set_ylabel(\"Shannon H\", fontsize=9, color=\"purple\")\n",
    "    ax_d.set_xlabel(\"Time\", fontsize=10)\n",
    "    ax_d.grid(True, lw=0.3, alpha=0.4)\n",
    "    h1, l1 = ax_d.get_legend_handles_labels()\n",
    "    h2, l2 = ax_d2.get_legend_handles_labels()\n",
    "    ax_d.legend(h1 + h2, l1 + l2, fontsize=7, loc=\"upper left\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Synthetic Microbiome Generator v3\\n\"\n",
    "    \"Ground Truth (latent gLV) vs Sequencing Observations (Dirichlet-Multinomial)\",\n",
    "    fontsize=13, fontweight=\"bold\", y=1.02)\n",
    "\n",
    "plt.savefig(\"/home/claude/demo_figure.png\", dpi=150, bbox_inches=\"tight\",\n",
    "            facecolor=\"white\")\n",
    "plt.close()\n",
    "print(\"\u2705 Demo figure saved.\")"
   ]
  }
 ]
}